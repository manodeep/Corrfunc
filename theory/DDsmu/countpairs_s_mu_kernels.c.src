// # -*- mode: c -*-
/* File: countpairs_s_mu_kernels.c.src */
/*
  This file is a part of the Corrfunc package
  Copyright (C) 2015-- Manodeep Sinha (manodeep@gmail.com)
  License: MIT LICENSE. See LICENSE file under the top-level
  directory at https://github.com/manodeep/Corrfunc/
*/


#include <stdio.h>
#include <stdlib.h>
#include <math.h>
#include <inttypes.h>

#include "function_precision.h"
#include "utils.h"

#include "weight_functions_DOUBLE.h"

#if defined(__AVX512F__)
#include "avx512_calls.h"

static inline int countpairs_s_mu_avx512_intrinsics_DOUBLE(const int64_t N0, DOUBLE *x0, DOUBLE *y0, DOUBLE *z0, const weight_struct_DOUBLE *weights0,
                                                           const int64_t N1, DOUBLE *x1, DOUBLE *y1, DOUBLE *z1, const weight_struct_DOUBLE *weights1,
                                                           const int same_cell,
                                                           const int fast_divide,
                                                           const DOUBLE sqr_smax, const DOUBLE sqr_smin, const int nsbin,
                                                           const int nmu_bins, const DOUBLE *supp_sqr, const DOUBLE mu_max, const DOUBLE pimax,
                                                           const DOUBLE off_xwrap, const DOUBLE off_ywrap, const DOUBLE off_zwrap,
                                                           DOUBLE *src_savg, uint64_t *src_npairs,
                                                           DOUBLE *src_weightavg, const weight_method_t weight_method)
{
    (void) sqr_smax, (void) sqr_smin;
    if(N0 == 0 || N1 == 0) {
        return EXIT_SUCCESS;
    }

    if(src_npairs == NULL) {
        return EXIT_FAILURE;
    }

    const int32_t need_savg = src_savg != NULL;
    const int32_t need_weightavg = src_weightavg != NULL;

    const int64_t totnbins = (nmu_bins+1)*(nsbin+1);
    uint64_t npairs[totnbins];
    DOUBLE savg[totnbins], weightavg[totnbins];
    for(int64_t i=0;i<totnbins;i++) {
        npairs[i] = 0;
        if(need_savg) {
            savg[i] = ZERO;
        }
        if(need_weightavg){
            weightavg[i] = ZERO;
        }
    }

    AVX512_FLOATS m_supp_sqr[nsbin];
    for(int i=0;i<nsbin;i++) {
        m_supp_sqr[i] = AVX512_SET_FLOAT(supp_sqr[i]);
    }

    const DOUBLE sqr_mumax = mu_max*mu_max;
    const DOUBLE dmu = mu_max/(DOUBLE) nmu_bins;
    const DOUBLE inv_dmu = 1.0/dmu;

    // A copy whose pointers we can advance
    weight_struct_DOUBLE local_w0 = {.weights={NULL}, .num_weights=0},
                         local_w1 = {.weights={NULL}, .num_weights=0};
    pair_struct_DOUBLE pair = {.num_weights=0};
    avx512_weight_func_t_DOUBLE avx512_weight_func = NULL;
    if(need_weightavg){
        // Same particle list, new copy of num_weights pointers into that list
        local_w0 = *weights0;
        local_w1 = *weights1;

        pair.num_weights = local_w0.num_weights;
        avx512_weight_func   = get_avx512_weight_func_by_method_DOUBLE(weight_method);
    }

    DOUBLE *zstart = z1;
    DOUBLE *zend = z1 + N1;
    for(int64_t i=0;i<N0;i++) {
        const DOUBLE xpos = *x0++ + off_xwrap;
        const DOUBLE ypos = *y0++ + off_ywrap;
        const DOUBLE zpos = *z0++ + off_zwrap;

        for(int w = 0; w < pair.num_weights; w++){
            // local_w0.weights[w] is a pointer to a float in the particle list of weights,
            // just as x0 is a pointer into the list of x-positions.
            // The advancement of the local_w0.weights[w] pointer should always mirror x0.
            pair.weights0[w].a512 = AVX512_SET_FLOAT(*(local_w0.weights[w])++);
        }

        if(same_cell == 1) {
            z1++;
        } else {
            while(z1 != zend && (*z1 - zpos) <= -pimax) {
                z1++;
            }
            if(z1 == zend) {
                i = N0;
                break;
            }
        }
        
        const int64_t n_off = z1 - zstart;
        DOUBLE *localz1 = z1;
        DOUBLE *localx1 = x1 + n_off;
        DOUBLE *localy1 = y1 + n_off;

        for(int w = 0; w < local_w1.num_weights; w++){
            local_w1.weights[w] = weights1->weights[w] + n_off;
        }
        
        for(int64_t j=n_off;j<N1;j+=AVX512_NVEC){
            AVX512_MASK m_mask_left = (N1 - j) >= AVX512_NVEC ? ~0:masks_per_misalignment_value_DOUBLE[N1-j];
            const AVX512_FLOATS m_x1 = AVX512_MASKZ_LOAD_FLOATS_UNALIGNED(m_mask_left, localx1);
            const AVX512_FLOATS m_y1 = AVX512_MASKZ_LOAD_FLOATS_UNALIGNED(m_mask_left, localy1);
            const AVX512_FLOATS m_z1 = AVX512_MASKZ_LOAD_FLOATS_UNALIGNED(m_mask_left, localz1);

            localx1 += AVX512_NVEC;//this might actually exceed the allocated range but we will never dereference that
            localy1 += AVX512_NVEC;
            localz1 += AVX512_NVEC;

            for(int w = 0; w < pair.num_weights; w++){
                pair.weights1[w].a512 = AVX512_MASKZ_LOAD_FLOATS_UNALIGNED(m_mask_left, local_w1.weights[w]);                
                local_w1.weights[w] += AVX512_NVEC;
            }
            
            const AVX512_FLOATS m_xpos    = AVX512_SET_FLOAT(xpos);
            const AVX512_FLOATS m_ypos    = AVX512_SET_FLOAT(ypos);
            const AVX512_FLOATS m_zpos    = AVX512_SET_FLOAT(zpos);

            union int16 union_finalbin;
            union float16 union_mDperp;
            union float16_weights union_mweight;

            const AVX512_FLOATS m_pimax = AVX512_SET_FLOAT((DOUBLE) pimax);
            const AVX512_FLOATS m_sqr_smax = m_supp_sqr[nsbin-1];
            const AVX512_FLOATS m_sqr_smin = m_supp_sqr[0];
            const AVX512_FLOATS m_sqr_mumax = AVX512_SET_FLOAT(sqr_mumax);

            const AVX512_FLOATS m_xdiff = AVX512_SUBTRACT_FLOATS(m_x1, m_xpos);  //(x[j] - x0)
            const AVX512_FLOATS m_ydiff = AVX512_SUBTRACT_FLOATS(m_y1, m_ypos);  //(y[j] - y0)
            const AVX512_FLOATS m_zdiff = AVX512_ABS_FLOAT(AVX512_SUBTRACT_FLOATS(m_z1, m_zpos));  //fabs(z2[j:j+NVEC-1] - z1)

            const AVX512_FLOATS m_sqr_zdiff = AVX512_SQUARE_FLOAT(m_zdiff);  //(z0 - z[j])^2
            const AVX512_FLOATS m_sqr_z_p_sqr_y = AVX512_FMA_ADD_FLOATS(m_ydiff, m_ydiff, m_sqr_zdiff);  //dy*dy + dz^2
            const AVX512_FLOATS s2  = AVX512_FMA_ADD_FLOATS(m_xdiff, m_xdiff, m_sqr_z_p_sqr_y);//s^2 = dx*dx + z^2 + dy^2

            const AVX512_FLOATS max_sqr_dz = AVX512_MULTIPLY_FLOATS(s2, m_sqr_mumax);

            //the z2 arrays are sorted in increasing order. which means
            //the z2 value will increase in any future iteration of j.
            //that implies the zdiff values are also monotonically increasing
            //Therefore, if none of the zdiff values are less than pimax, then
            //no future iteration in j can produce a zdiff value less than pimax.
            const AVX512_MASK m_mask_pimax = AVX512_MASK_COMPARE_FLOATS(m_mask_left, m_zdiff,m_pimax,_CMP_LT_OS);
            if(m_mask_pimax == 0) {
                j=N1;
                break;
            }
            
            const AVX512_MASK m_mu_mask = AVX512_MASK_COMPARE_FLOATS(m_mask_pimax, m_sqr_zdiff, max_sqr_dz, _CMP_LT_OS);
            const AVX512_MASK m_smax_mask = AVX512_MASK_COMPARE_FLOATS(m_mask_pimax, s2, m_sqr_smax, _CMP_LT_OS);//check for s2 < sqr_smax
            const AVX512_MASK m_smin_mask = AVX512_MASK_COMPARE_FLOATS(m_mask_pimax, s2, m_sqr_smin, _CMP_GE_OS);//check for s2 >= sqr_smin
            const AVX512_MASK m_s2_mask = AVX512_MASK_BITWISE_AND(m_smax_mask, m_smin_mask);
            
            //Create a combined mask by bitwise and of m1 and m_mask_left.
            //This gives us the mask for all sqr_smin <= s2 < sqr_smax
            // + mu_min <= mu < mu_max
            m_mask_left = AVX512_MASK_BITWISE_AND(m_mu_mask, m_s2_mask);
            
            //If not, continue with the next iteration of j-loop
            if(m_mask_left == 0) {
                continue;
            }

            /*m_mu := sqrt(dz^2/s2) (with masked elements set to mu_max */
            AVX512_FLOATS m_sqr_mu = AVX512_SETZERO_FLOAT();
            if(fast_divide == 0) {
                //regular division -> slow op
                m_sqr_mu = AVX512_MASKZ_DIVIDE_FLOATS(m_mask_left, m_sqr_zdiff, s2);
                //The divide is the actual operation we need
                // but divides are about 10x slower than multiplies. So, I am replacing it
                //with a approximate reciprocal in floating point
                // + 2 iterations of newton-raphson in case of DOUBLE
            } else {
                //following blocks do an approximate reciprocal followed by two iterations of Newton-Raphson
                const AVX512_FLOATS rc = AVX512_MASKZ_RECIPROCAL_FLOATS(m_mask_left, s2);
                //We have the double->float->approx. reciprocal->double process done.
                //Now improve the accuracy of the divide with newton-raphson.
                
                //Ist iteration of NewtonRaphson
                const AVX512_FLOATS two = AVX512_SET_FLOAT((DOUBLE) 2.0);
                const AVX512_FLOATS rc1 = AVX512_MULTIPLY_FLOATS(rc,
                                                                 AVX512_FNMA_ADD_FLOATS(s2, rc, two));/*2.0 - l^2*rc */
                //2nd iteration of NewtonRaphson
                const AVX512_FLOATS rc2 = AVX512_MULTIPLY_FLOATS(rc1,
                                                                 AVX512_FNMA_ADD_FLOATS(s2, rc1, two));/* 2.0 - l^2*rc1 */
                
                m_sqr_mu = AVX512_MASKZ_MULTIPLY_FLOATS(m_mask_left, m_sqr_zdiff, rc2);
            }//end of FAST_DIVIDE
#endif            
            const AVX512_FLOATS m_mu = AVX512_MASKZ_SQRT_FLOAT(m_mask_left, m_sqr_mu);
            if(need_savg) {
                union_mDperp.m_Dperp = AVX512_MASKZ_SQRT_FLOAT(m_mask_left, s2);
            }
            if(need_weightavg){
                pair.dx.a512 = m_xdiff;
                pair.dy.a512 = m_ydiff;
                pair.dz.a512 = m_zdiff;

                union_mweight.m_weights = avx512_weight_func(&pair);
            }

            const AVX512_MASK m_mask = m_mask_left;
            AVX512_FLOATS m_sbin     = AVX512_SETZERO_FLOAT();
            for(int kbin=nsbin-1;kbin>=1;kbin--) {
                const AVX512_MASK m_bin_mask = AVX512_MASK_COMPARE_FLOATS(m_mask_left, s2, m_supp_sqr[kbin-1],_CMP_GE_OQ);
                m_sbin = AVX512_BLEND_FLOATS_WITH_MASK(m_bin_mask, m_sbin, AVX512_SET_FLOAT(kbin));
                m_mask_left = AVX512_MASK_BITWISE_AND_NOT(m_bin_mask, m_mask_left);//ANDNOT(X, Y) -> NOT X AND Y
                if(m_mask_left == 0) {
                    break;
                }
            }
            const AVX512_FLOATS m_inv_dmu    = AVX512_SET_FLOAT(inv_dmu);
            const AVX512_FLOATS m_nmu_bins = AVX512_SET_FLOAT((DOUBLE) nmu_bins);
            const AVX512_FLOATS m_nmu_bins_p1 = AVX512_SET_FLOAT((DOUBLE) (nmu_bins + 1));
            const AVX512_FLOATS m_mubin = AVX512_MULTIPLY_FLOATS(m_mu,m_inv_dmu);

            /* Compute the 1-D index to the [sbin, mubin] := sbin*(nmubin+1) + mubin */
            const AVX512_FLOATS m_sbin_linear_index = AVX512_MULTIPLY_FLOATS(m_sbin, m_nmu_bins_p1);
            const AVX512_FLOATS m_finalbin = AVX512_MASK_ADD_FLOATS(m_nmu_bins, m_mask, m_sbin_linear_index, m_mubin);
            union_finalbin.m_ibin = AVX512_TRUNCATE_FLOAT_TO_INT(m_finalbin);

            //update the histograms
#if defined(__ICC) || defined(__INTEL_COMPILER)
#pragma unroll(AVX512_NVEC)
#endif
            for(int jj=0;jj<AVX512_NVEC;jj++) {
                int ibin = union_finalbin.ibin[jj];
                npairs[ibin]++;
                if(need_savg) {
                    savg[ibin] += union_mDperp.Dperp[jj];
                }
                if(need_weightavg){
                    const DOUBLE weight = union_mweight.weights[jj];
                    weightavg[ibin] += weight;
                }
            }
        }//loop over second set of particles
    }//loop over first set of particles

	for(int i=0;i<totnbins;i++) {
		src_npairs[i] += npairs[i];
        if(need_savg) {
            src_savg[i] += savg[i];
        }
        if(need_weightavg) {
            src_weightavg[i] += weightavg[i];
        }
    }

    return EXIT_SUCCESS;
}
#endif //__AVX512F__


#if defined(__AVX__)
#include "avx_calls.h"

static inline int countpairs_s_mu_avx_intrinsics_DOUBLE(const int64_t N0, DOUBLE *x0, DOUBLE *y0, DOUBLE *z0, const weight_struct_DOUBLE *weights0,
                                                        const int64_t N1, DOUBLE *x1, DOUBLE *y1, DOUBLE *z1, const weight_struct_DOUBLE *weights1,
                                                        const int same_cell,
                                                        const int fast_divide,
                                                        const DOUBLE sqr_smax, const DOUBLE sqr_smin, const int nsbin,
                                                        const int nmu_bins, const DOUBLE *supp_sqr, const DOUBLE mu_max, const DOUBLE pimax,
                                                        const DOUBLE off_xwrap, const DOUBLE off_ywrap, const DOUBLE off_zwrap,
                                                        DOUBLE *src_savg, uint64_t *src_npairs,
                                                        DOUBLE *src_weightavg, const weight_method_t weight_method)
{
    if(N0 == 0 || N1 == 0) {
        return EXIT_SUCCESS;
    }

    if(src_npairs == NULL) {
        return EXIT_FAILURE;
    }

    const int32_t need_savg = src_savg != NULL;
    const int32_t need_weightavg = src_weightavg != NULL;

    const int64_t totnbins = (nmu_bins+1)*(nsbin+1);
    uint64_t npairs[totnbins];
    DOUBLE savg[totnbins], weightavg[totnbins];
    for(int64_t i=0;i<totnbins;i++) {
        npairs[i] = 0;
        if(need_savg) {
            savg[i] = ZERO;
        }
        if(need_weightavg){
            weightavg[i] = ZERO;
        }
    }


    AVX_FLOATS m_supp_sqr[nsbin];
    AVX_FLOATS m_kbin[nsbin];
    for(int i=0;i<nsbin;i++) {
        m_supp_sqr[i] = AVX_SET_FLOAT(supp_sqr[i]);
        m_kbin[i] = AVX_SET_FLOAT((DOUBLE) i);
    }

    /* const AVX_FLOATS m_mumax = AVX_SET_FLOAT(mu_max); */
    const DOUBLE sqr_mumax = mu_max*mu_max;
    const DOUBLE dmu = mu_max/(DOUBLE) nmu_bins;
    const DOUBLE inv_dmu = 1.0/dmu;

    // A copy whose pointers we can advance
    weight_struct_DOUBLE local_w0 = {.weights={NULL}, .num_weights=0},
                         local_w1 = {.weights={NULL}, .num_weights=0};
    pair_struct_DOUBLE pair = {.num_weights=0};
    avx_weight_func_t_DOUBLE avx_weight_func = NULL;
    weight_func_t_DOUBLE fallback_weight_func = NULL;
    if(need_weightavg){
        // Same particle list, new copy of num_weights pointers into that list
        local_w0 = *weights0;
        local_w1 = *weights1;

        pair.num_weights = local_w0.num_weights;

        avx_weight_func = get_avx_weight_func_by_method_DOUBLE(weight_method);
        fallback_weight_func = get_weight_func_by_method_DOUBLE(weight_method);
    }

    int64_t prev_j = 0, n_off = 0;
    for(int64_t i=0;i<N0;i++) {
        const DOUBLE xpos = *x0++ + off_xwrap;
        const DOUBLE ypos = *y0++ + off_ywrap;
        const DOUBLE zpos = *z0++ + off_zwrap;
        for(int w = 0; w < pair.num_weights; w++){
            // local_w0.weights[w] is a pointer to a float in the particle list of weights,
            // just as x0 is a pointer into the list of x-positions.
            // The advancement of the local_w0.weights[w] pointer should always mirror x0.
            pair.weights0[w].a = AVX_SET_FLOAT(*(local_w0.weights[w])++);
        }

        int64_t j;
        if(same_cell == 1) {
            z1++; n_off++;
            j = i+1;
        } else {
            for(;prev_j<N1;prev_j++) {
                const DOUBLE dz = *z1 - zpos;
                if(dz > -pimax) break;
                z1++; n_off++;
            }
            if(prev_j == N1) {
                i = N0;
                break;
            }
            j = prev_j;
        }
        DOUBLE *localz1 = z1;
        DOUBLE *localx1 = x1 + n_off;
        DOUBLE *localy1 = y1 + n_off;
        for(int w = 0; w < local_w1.num_weights; w++){
            local_w1.weights[w] = weights1->weights[w] + n_off;
        }

        for(;j<=(N1 - AVX_NVEC);j+=AVX_NVEC) {
            const AVX_FLOATS m_xpos    = AVX_SET_FLOAT(xpos);
            const AVX_FLOATS m_ypos    = AVX_SET_FLOAT(ypos);
            const AVX_FLOATS m_zpos    = AVX_SET_FLOAT(zpos);

            union int8 union_finalbin;
            union float8 union_mDperp;
            union float8_weights union_mweight;

            const AVX_FLOATS m_x1 = AVX_LOAD_FLOATS_UNALIGNED(localx1);
            const AVX_FLOATS m_y1 = AVX_LOAD_FLOATS_UNALIGNED(localy1);
            const AVX_FLOATS m_z1 = AVX_LOAD_FLOATS_UNALIGNED(localz1);

            localx1 += AVX_NVEC;//this might actually exceed the allocated range but we will never dereference that
            localy1 += AVX_NVEC;
            localz1 += AVX_NVEC;

            for(int w = 0; w < pair.num_weights; w++){
                pair.weights1[w].a = AVX_LOAD_FLOATS_UNALIGNED(local_w1.weights[w]);
                local_w1.weights[w] += AVX_NVEC;
            }

            const AVX_FLOATS m_pimax = AVX_SET_FLOAT((DOUBLE) pimax);
            const AVX_FLOATS m_sqr_smax = m_supp_sqr[nsbin-1];
            const AVX_FLOATS m_sqr_smin = m_supp_sqr[0];
            const AVX_FLOATS m_inv_dmu    = AVX_SET_FLOAT(inv_dmu);
            const AVX_FLOATS m_sqr_mumax = AVX_SET_FLOAT(sqr_mumax);

            const AVX_FLOATS m_zero = AVX_SETZERO_FLOAT();
            const AVX_FLOATS m_nmu_bins     = AVX_SET_FLOAT((DOUBLE) nmu_bins);
            const AVX_FLOATS m_one    = AVX_SET_FLOAT((DOUBLE) 1);

            const AVX_FLOATS m_xdiff = AVX_SUBTRACT_FLOATS(m_x1, m_xpos);  //(x[j] - x0)
            const AVX_FLOATS m_ydiff = AVX_SUBTRACT_FLOATS(m_y1, m_ypos);  //(y[j] - y0)
            AVX_FLOATS m_zdiff = AVX_SUBTRACT_FLOATS(m_z1, m_zpos);  //z2[j:j+NVEC-1] - z1

            const AVX_FLOATS m_sqr_xdiff = AVX_SQUARE_FLOAT(m_xdiff);  //(x0 - x[j])^2
            const AVX_FLOATS m_sqr_ydiff = AVX_SQUARE_FLOAT(m_ydiff);  //(y0 - y[j])^2
            const AVX_FLOATS m_sqr_zdiff = AVX_SQUARE_FLOAT(m_zdiff);  //(z0 - z[j])^2

            AVX_FLOATS s2  = AVX_ADD_FLOATS(m_sqr_zdiff, AVX_ADD_FLOATS(m_sqr_xdiff, m_sqr_ydiff));//s^2 = dz^2 + dx^2 + dy^2
            m_zdiff = AVX_MAX_FLOATS(m_zdiff,AVX_SUBTRACT_FLOATS(m_zero,m_zdiff));//dz = fabs(dz) => dz = max(dz, -dz);

            AVX_FLOATS m_mask_left;
            AVX_FLOATS max_sqr_dz = AVX_MULTIPLY_FLOATS(s2, m_sqr_mumax);

            //Do all the distance cuts using masks here in new scope
            {
                //the z2 arrays are sorted in increasing order. which means
                //the z2 value will increase in any future iteration of j.
                //that implies the zdiff values are also monotonically increasing
                //Therefore, if none of the zdiff values are less than pimax, then
                //no future iteration in j can produce a zdiff value less than pimax.
                AVX_FLOATS m_mask_pimax = AVX_COMPARE_FLOATS(m_zdiff,m_pimax,_CMP_LT_OS);
                if(AVX_TEST_COMPARISON(m_mask_pimax) == 0) {
                    j=N1;
                    break;
                }

                const AVX_FLOATS m_mu_mask = AVX_COMPARE_FLOATS(m_sqr_zdiff, max_sqr_dz, _CMP_LT_OS);
                const AVX_FLOATS m_smax_mask = AVX_COMPARE_FLOATS(s2, m_sqr_smax, _CMP_LT_OS);//check for s2 < sqr_smax
                const AVX_FLOATS m_smin_mask = AVX_COMPARE_FLOATS(s2, m_sqr_smin, _CMP_GE_OS);//check for s2 >= sqr_smin
                const AVX_FLOATS m_s2_mask = AVX_BITWISE_AND(m_smax_mask,m_smin_mask);

                //Create a combined mask by bitwise and of m1 and m_mask_left.
                //This gives us the mask for all sqr_smin <= s2 < sqr_smax
                // + mu_min <= mu < mu_max
                m_mask_left = AVX_BITWISE_AND(m_mu_mask, m_s2_mask);

                //If not, continue with the next iteration of j-loop
                if(AVX_TEST_COMPARISON(m_mask_left) == 0) {
                    continue;
                }

            }

            //There is some s2 that satisfies sqr_smin <= s2 < sqr_smax && mu_min <= |dz| < mu_max
            s2 = AVX_BLEND_FLOATS_WITH_MASK(m_sqr_smax, s2, m_mask_left);
            /*m_sqr_mu := dz^2/s^2 (with masked elements set to mu_max */
            AVX_FLOATS m_sqr_mu = AVX_SETZERO_FLOAT();
            if(fast_divide == 0) {
                //regular division -> slow op
                m_sqr_mu = AVX_DIVIDE_FLOATS(m_sqr_zdiff, s2);    
                //The divide is the actual operation we need
                // but divides are about 10x slower than multiplies. So, I am replacing it
                //with a approximate reciprocal in floating point
                // + 2 iterations of newton-raphson in case of DOUBLE
            } else {
                //following blocks do an approximate reciprocal followed by two iterations of Newton-Raphson
                //However, the exact implementation depends on the precision. floats have an inbuilt approx. reciprocal
                //but doubles do not. So, we have to 'fake' an approximate reciprocal for doubles by converting to float
                //taking the approximate reciprocal, and then convert back to double
#ifndef DOUBLE_PREC
                const AVX_FLOATS rc  = _mm256_rcp_ps(s2);//intrinsic for 256 bit approximate reciprocal
#else
                //we have to do this for doubles now.
                //if the vrcpps instruction is not generated, there will
                //be a ~70 cycle performance hit from switching between
                //AVX and SSE modes.
                const __m128 float_tmp1 =  _mm256_cvtpd_ps(s2);//convert double to float -> not avx_floats := _m256d 
                //(convert 4 doubles into 4 floats -> use half of available 256 bit SIMD registers)
                __m128 float_inv_tmp1 = _mm_rcp_ps(float_tmp1);//intrinsic for 128 bit float approximate reciprocal
                const AVX_FLOATS rc = _mm256_cvtps_pd(float_inv_tmp1);//convert back to double
#endif//DOUBLE_PREC
                
                //We have the double->float->approx. reciprocal->double process done.
                //Now improve the accuracy of the divide with newton-raphson.
                
                //Ist iteration of NewtonRaphson
                const AVX_FLOATS two = AVX_SET_FLOAT((DOUBLE) 2.0);
                const AVX_FLOATS rc1 = AVX_MULTIPLY_FLOATS(rc,
                                                           AVX_SUBTRACT_FLOATS(two,
                                                                               AVX_MULTIPLY_FLOATS(s2,rc)));
                //2nd iteration of NewtonRaphson
                const AVX_FLOATS rc2 = AVX_MULTIPLY_FLOATS(rc1,
                                                           AVX_SUBTRACT_FLOATS(two,
                                                                               AVX_MULTIPLY_FLOATS(2,rc1)));
                m_sqr_mu = AVX_MULTIPLY_FLOATS(m_sqr_zdiff,rc2);
            }//end of FAST_DIVIDE
                
            const AVX_FLOATS m_mu = AVX_SQRT_FLOAT(AVX_BLEND_FLOATS_WITH_MASK(m_sqr_mumax, m_sqr_mu, m_mask_left));
            
            if(need_savg) {
                union_mDperp.m_Dperp = AVX_SQRT_FLOAT(s2);
            }
            if(need_weightavg){
                pair.dx.a = m_xdiff;
                pair.dy.a = m_ydiff;
                pair.dz.a = m_zdiff;

                union_mweight.m_weights = avx_weight_func(&pair);
            }

            const AVX_FLOATS m_mubin = AVX_MULTIPLY_FLOATS(m_mu,m_inv_dmu);
            AVX_FLOATS m_sbin     = AVX_SETZERO_FLOAT();
            //AVX_FLOATS m_all_ones  = AVX_CAST_INT_TO_FLOAT(AVX_SET_INT(-1));
            for(int kbin=nsbin-1;kbin>=1;kbin--) {
                const AVX_FLOATS m_mask_low = AVX_COMPARE_FLOATS(s2,m_supp_sqr[kbin-1],_CMP_GE_OS);
                const AVX_FLOATS m_bin_mask = AVX_BITWISE_AND(m_mask_low,m_mask_left);
                m_sbin = AVX_BLEND_FLOATS_WITH_MASK(m_sbin,m_kbin[kbin], m_bin_mask);
                m_mask_left = AVX_COMPARE_FLOATS(s2, m_supp_sqr[kbin-1],_CMP_LT_OS);
                //m_mask_left = AVX_XOR_FLOATS(m_mask_low, m_all_ones);//XOR with 0xFFFF... gives the bins that are smaller than m_supp_sqr[kbin] (and is faster than cmp_p(s/d) in theory)
                const int test = AVX_TEST_COMPARISON(m_mask_left);
                if(test==0) {
                    break;
                }
            }
            const AVX_FLOATS m_nmu_bins_p1 = AVX_ADD_FLOATS(m_nmu_bins,m_one);
            const AVX_FLOATS m_binproduct = AVX_ADD_FLOATS(AVX_MULTIPLY_FLOATS(m_sbin,m_nmu_bins_p1),m_mubin);
            union_finalbin.m_ibin = AVX_TRUNCATE_FLOAT_TO_INT(m_binproduct);

            //update the histograms
#if defined(__ICC) || defined(__INTEL_COMPILER)
#pragma unroll(AVX_NVEC)
#endif
            for(int jj=0;jj<AVX_NVEC;jj++) {
                int ibin = union_finalbin.ibin[jj];
                npairs[ibin]++;
                if(need_savg) {
                    savg[ibin] += union_mDperp.Dperp[jj];
                }
                if(need_weightavg){
                    const DOUBLE weight = union_mweight.weights[jj];
                    weightavg[ibin] += weight;
                }
            }
        }


        //remainder loop
        for(;j<N1;j++){
            const DOUBLE dz = FABS(*localz1++ - zpos);
            const DOUBLE dx = *localx1++ - xpos;
            const DOUBLE dy = *localy1++ - ypos;
            for(int w = 0; w < pair.num_weights; w++){
                pair.weights1[w].d = *local_w1.weights[w]++;
            }

            if(dz >= pimax) {
                break;
            }

            const DOUBLE sqr_dx_dy = dx*dx + dy*dy;
            const DOUBLE sqr_dz = dz*dz;
            const DOUBLE s2 =  sqr_dx_dy + sqr_dz;
            if(s2 >= sqr_smax || s2 < sqr_smin)
                continue;
            if(sqr_dz >= s2 * sqr_mumax) continue;
            const DOUBLE mu = SQRT(sqr_dz/s2);

            DOUBLE s, pairweight;
            if(need_savg) {
                s = SQRT(s2);
            }
            if(need_weightavg){
                pair.dx.d = dx;
                pair.dy.d = dy;
                pair.dz.d = dz;
                pairweight = fallback_weight_func(&pair);
            }

            int mu_bin = (int) (mu*inv_dmu);
            mu_bin = mu_bin > nmu_bins ? nmu_bins:mu_bin;
            for(int kbin=nsbin-1;kbin>=1;kbin--) {
                if(s2 >= supp_sqr[kbin-1]) {
                    const int ibin = kbin*(nmu_bins+1) + mu_bin;
                    npairs[ibin]++;
                    if(need_savg) {
                        savg[ibin] += s;
                    }
                    if(need_weightavg){
                        weightavg[ibin] += pairweight;
                    }
                    break;
                }
            }
        }//remainder loop over second set of particles
    }//loop over first set of particles

	for(int i=0;i<totnbins;i++) {
		src_npairs[i] += npairs[i];
        if(need_savg) {
            src_savg[i] += savg[i];
        }
        if(need_weightavg) {
            src_weightavg[i] += weightavg[i];
        }
    }

    return EXIT_SUCCESS;
}
#endif //__AVX__



#if defined (__SSE4_2__)
#include "sse_calls.h"

static inline int countpairs_s_mu_sse_intrinsics_DOUBLE(const int64_t N0, DOUBLE *x0, DOUBLE *y0, DOUBLE *z0, const weight_struct_DOUBLE *weights0,
                                                        const int64_t N1, DOUBLE *x1, DOUBLE *y1, DOUBLE *z1, const weight_struct_DOUBLE *weights1,
                                                        const int same_cell,
                                                        const int fast_divide,
                                                        const DOUBLE sqr_smax, const DOUBLE sqr_smin, const int nsbin, const int nmu_bins,
                                                        const DOUBLE *supp_sqr, const DOUBLE mu_max, const DOUBLE pimax,
                                                        const DOUBLE off_xwrap, const DOUBLE off_ywrap, const DOUBLE off_zwrap,
                                                        DOUBLE *src_savg, uint64_t *src_npairs,
                                                        DOUBLE *src_weightavg, const weight_method_t weight_method)
{
    (void) fast_divide;
    
    if(N0 == 0 || N1 == 0) {
        return EXIT_SUCCESS;
    }

    if(src_npairs == NULL) {
        return EXIT_FAILURE;
    }

    const int32_t need_savg = src_savg != NULL;
    const int32_t need_weightavg = src_weightavg != NULL;
    const int64_t totnbins = (nmu_bins+1) * (nsbin+1);
    uint64_t npairs[totnbins];
    DOUBLE savg[totnbins], weightavg[totnbins];
    for(int64_t i=0;i<totnbins;i++) {
        npairs[i] = 0;
        if (need_savg) {
            savg[i] = ZERO;
        }
        if(need_weightavg){
            weightavg[i] = ZERO;
        }
    }

    SSE_FLOATS m_kbin[nsbin];
    SSE_FLOATS m_supp_sqr[nsbin];
    for(int i=0;i<nsbin;i++) {
        m_kbin[i] = SSE_SET_FLOAT((DOUBLE) i);
        m_supp_sqr[i] = SSE_SET_FLOAT(supp_sqr[i]);
    }

    const DOUBLE sqr_mumax = mu_max*mu_max;
    const DOUBLE dmu = mu_max/(DOUBLE) nmu_bins;
    const DOUBLE inv_dmu = 1.0/dmu;


    // A copy whose pointers we can advance
    weight_struct_DOUBLE local_w0 = {.weights={NULL}, .num_weights=0},
                         local_w1 = {.weights={NULL}, .num_weights=0};
    pair_struct_DOUBLE pair = {.num_weights=0};
    sse_weight_func_t_DOUBLE sse_weight_func = NULL;
    weight_func_t_DOUBLE fallback_weight_func = NULL;
    if(need_weightavg){
      // Same particle list, new copy of num_weights pointers into that list
      local_w0 = *weights0;
      local_w1 = *weights1;

      pair.num_weights = local_w0.num_weights;

      sse_weight_func = get_sse_weight_func_by_method_DOUBLE(weight_method);
      fallback_weight_func = get_weight_func_by_method_DOUBLE(weight_method);
    }

    int64_t prev_j = 0, n_off = 0;
    for(int64_t i=0;i<N0;i++) {
        const DOUBLE xpos = *x0++ + off_xwrap;
        const DOUBLE ypos = *y0++ + off_ywrap;
        const DOUBLE zpos = *z0++ + off_zwrap;
        for(int w = 0; w < pair.num_weights; w++){
            // local_w0.weights[w] is a pointer to a float in the particle list of weights,
            // just as x0 is a pointer into the list of x-positions.
            // The advancement of the local_w0.weights[w] pointer should always mirror x0.
            pair.weights0[w].s = SSE_SET_FLOAT(*local_w0.weights[w]++);
        }

        int64_t j;
        if(same_cell == 1) {
            z1++; n_off++;
            j = i+1;
        } else {
            for(;prev_j<N1;prev_j++) {
                const DOUBLE dz = *z1 - zpos;
                if(dz > -pimax) break;
                z1++; n_off++;
            }
            if(prev_j == N1) {
                i = N0;
                break;
            }
            j = prev_j;
        }
        DOUBLE *localz1 = z1;
        DOUBLE *localx1 = x1 + n_off;
        DOUBLE *localy1 = y1 + n_off;
        for(int w = 0; w < local_w1.num_weights; w++){
            local_w1.weights[w] = weights1->weights[w] + n_off;
        }

        for(;j<=(N1 - SSE_NVEC);j+=SSE_NVEC){

            union int4{
                SSE_INTS m_ibin;
                int ibin[SSE_NVEC];
            };
            union int4 union_finalbin;

            union float4{
                SSE_FLOATS m_Dperp;
                DOUBLE Dperp[SSE_NVEC];
            };
            union float4 union_mDperp;

            const SSE_FLOATS m_xpos = SSE_SET_FLOAT(xpos);
            const SSE_FLOATS m_ypos = SSE_SET_FLOAT(ypos);
            const SSE_FLOATS m_zpos = SSE_SET_FLOAT(zpos);

            const SSE_FLOATS m_x1 = SSE_LOAD_FLOATS_UNALIGNED(localx1);
            const SSE_FLOATS m_y1 = SSE_LOAD_FLOATS_UNALIGNED(localy1);
            const SSE_FLOATS m_z1 = SSE_LOAD_FLOATS_UNALIGNED(localz1);

            localx1 += SSE_NVEC;
            localy1 += SSE_NVEC;
            localz1 += SSE_NVEC;

            for(int w = 0; w < pair.num_weights; w++){
                pair.weights1[w].s = SSE_LOAD_FLOATS_UNALIGNED(local_w1.weights[w]);
                local_w1.weights[w] += SSE_NVEC;
            }

            union float4_weights{
                SSE_FLOATS m_weights;
                DOUBLE weights[SSE_NVEC];
            };
            union float4_weights union_mweight;

            const SSE_FLOATS m_pimax = SSE_SET_FLOAT((DOUBLE) pimax);
            const SSE_FLOATS m_sqr_smax = m_supp_sqr[nsbin-1];
            const SSE_FLOATS m_sqr_smin = m_supp_sqr[0];
            const SSE_FLOATS m_sqr_mumax = SSE_SET_FLOAT(sqr_mumax);
            const SSE_FLOATS m_inv_dmu    = SSE_SET_FLOAT(inv_dmu);
            const SSE_FLOATS m_zero = SSE_SETZERO_FLOAT();
            const SSE_FLOATS m_nmu_bins     = SSE_SET_FLOAT((DOUBLE) nmu_bins);
            const SSE_FLOATS m_one    = SSE_SET_FLOAT((DOUBLE) 1);

            const SSE_FLOATS m_xdiff = SSE_SUBTRACT_FLOATS(m_x1, m_xpos);  //(x[j] - x0)
            const SSE_FLOATS m_ydiff = SSE_SUBTRACT_FLOATS(m_y1, m_ypos);  //(y[j] - y0)
            SSE_FLOATS m_zdiff = SSE_SUBTRACT_FLOATS(m_z1, m_zpos);  //z2[j:j+NVEC-1] - z1

            const SSE_FLOATS m_sqr_xdiff = SSE_SQUARE_FLOAT(m_xdiff);
            const SSE_FLOATS m_sqr_ydiff = SSE_SQUARE_FLOAT(m_ydiff);
            const SSE_FLOATS m_sqr_zdiff = SSE_SQUARE_FLOAT(m_zdiff);

            SSE_FLOATS s2  = SSE_ADD_FLOATS(m_sqr_zdiff, SSE_ADD_FLOATS(m_sqr_xdiff, m_sqr_ydiff));//s^2 = dx^2 + dy^2 + dz^2
            m_zdiff = SSE_MAX_FLOATS(m_zdiff,SSE_SUBTRACT_FLOATS(m_zero,m_zdiff));//dz = fabs(dz) => dz = max(dz, -dz);

            SSE_FLOATS m_mask_left;
            SSE_FLOATS max_sqr_dz = SSE_MULTIPLY_FLOATS(s2, m_sqr_mumax);

            //Do all the distance cuts using masks here in new scope
            {
                //the z2 arrays are sorted in increasing order. which means
                //the z2 value will increase in any future iteration of j.
                //that implies the zdiff values are also monotonically increasing
                //Therefore, if none of the zdiff values are less than pimax, then
                //no future iteration in j can produce a zdiff value less than pimax.
                SSE_FLOATS m_mask_pimax = SSE_COMPARE_FLOATS_LT(m_zdiff,m_pimax);
                if(SSE_TEST_COMPARISON(m_mask_pimax) == 0) {
                    j=N1;
                    break;
                }

                const SSE_FLOATS m_mu_mask = SSE_COMPARE_FLOATS_LT(m_sqr_zdiff, max_sqr_dz);
                const SSE_FLOATS m_smax_mask = SSE_COMPARE_FLOATS_LT(s2, m_sqr_smax);
                const SSE_FLOATS m_smin_mask = SSE_COMPARE_FLOATS_GE(s2, m_sqr_smin);
                const SSE_FLOATS m_s2_mask = SSE_BITWISE_AND(m_smax_mask,m_smin_mask);

                //Create a combined mask by bitwise and of m1 and m_mask_left.
                //This gives us the mask for all sqr_smin <= s2 < sqr_smax
                // + mu_min <= mu < mu_max
                m_mask_left = SSE_BITWISE_AND(m_mu_mask, m_s2_mask);

                //If not, continue with the next iteration of j-loop
                if(SSE_TEST_COMPARISON(m_mask_left) == 0) {
                    continue;
                }

            }

            //There is some s2 that satisfies sqr_smin <= s2 < sqr_smax && mu_min <= |dz| < mu_max
            s2 = SSE_BLEND_FLOATS_WITH_MASK(m_sqr_smax, s2, m_mask_left);
            const SSE_FLOATS m_mu = SSE_SQRT_FLOAT(SSE_BLEND_FLOATS_WITH_MASK(m_sqr_mumax, SSE_DIVIDE_FLOATS(m_sqr_zdiff, s2), m_mask_left));

            if(need_savg) {
                union_mDperp.m_Dperp = SSE_SQRT_FLOAT(s2);
            }
            if(need_weightavg){
                pair.dx.s = m_xdiff;
                pair.dy.s = m_ydiff;
                pair.dz.s = m_zdiff;

                union_mweight.m_weights = sse_weight_func(&pair);
            }

            const SSE_FLOATS m_mubin = SSE_MULTIPLY_FLOATS(m_mu,m_inv_dmu);
            SSE_FLOATS m_sbin     = SSE_SETZERO_FLOAT();
            //SSE_FLOATS m_all_ones  = SSE_CAST_INT_TO_FLOAT(SSE_SET_INT(-1));
            for(int kbin=nsbin-1;kbin>=1;kbin--) {
                const SSE_FLOATS m_mask_low = SSE_COMPARE_FLOATS_GE(s2,m_supp_sqr[kbin-1]);
                const SSE_FLOATS m_bin_mask = SSE_BITWISE_AND(m_mask_low,m_mask_left);
                m_sbin = SSE_BLEND_FLOATS_WITH_MASK(m_sbin,m_kbin[kbin], m_bin_mask);
                m_mask_left = SSE_COMPARE_FLOATS_LT(s2, m_supp_sqr[kbin-1]);
                //XOR with 0xFFFF... gives the bins that are smaller than m_supp_sqr[kbin] (and is faster than cmp_p(s/d) in theory)
                //m_mask_left = SSE_XOR_FLOATS(m_mask_low, m_all_ones);
                const int test = SSE_TEST_COMPARISON(m_mask_left);
                if(test==0) {
                    break;
                }
            }
            const SSE_FLOATS m_nmu_bins_p1 = SSE_ADD_FLOATS(m_nmu_bins,m_one);
            const SSE_FLOATS m_binproduct = SSE_ADD_FLOATS(SSE_MULTIPLY_FLOATS(m_sbin,m_nmu_bins_p1),m_mubin);
            union_finalbin.m_ibin = SSE_TRUNCATE_FLOAT_TO_INT(m_binproduct);

            //update the histograms
#if defined(__ICC) || defined(__INTEL_COMPILER)
#pragma unroll(SSE_NVEC)
#endif
            for(int jj=0;jj<SSE_NVEC;jj++) {
                int ibin = union_finalbin.ibin[jj];
                npairs[ibin]++;
                if(need_savg) {
                    savg[ibin] += union_mDperp.Dperp[jj];
                }
                if(need_weightavg){
                    const DOUBLE weight = union_mweight.weights[jj];
                    weightavg[ibin] += weight;
                }
            }
        }


        for(;j<N1;j++) {
            const DOUBLE dx = *localx1++ - xpos;
            const DOUBLE dy = *localy1++ - ypos;
            const DOUBLE dz = FABS(*localz1++ - zpos);
            for(int w = 0; w < pair.num_weights; w++){
                pair.weights1[w].d = *local_w1.weights[w]++;
            }

            if(dz >= pimax) break;

            const DOUBLE sqr_dx_dy = dx*dx + dy*dy;
            const DOUBLE sqr_dz = dz*dz;
            const DOUBLE s2 =  sqr_dx_dy + sqr_dz;
            if(s2 >= sqr_smax || s2 < sqr_smin)
                continue;
            if(sqr_dz >= s2 * sqr_mumax) continue;
            const DOUBLE mu = SQRT(sqr_dz/s2);

            DOUBLE s, pairweight;            
            if(need_weightavg){
                pair.dx.d = dx;
                pair.dy.d = dy;
                pair.dz.d = dz;
                pairweight = fallback_weight_func(&pair);                
            }

            if(need_savg) {
                s = SQRT(s2);
            }

            int mu_bin = (int) (mu*inv_dmu);
            mu_bin = mu_bin > nmu_bins ? nmu_bins:mu_bin;
            for(int kbin=nsbin-1;kbin>=1;kbin--) {
                if(s2 >= supp_sqr[kbin-1]) {
                    const int ibin = kbin*(nmu_bins+1) + mu_bin;
                    npairs[ibin]++;
                    if(need_savg) {
                        savg[ibin] += s;
                    }
                    if(need_weightavg){
                        weightavg[ibin] += pairweight;
                    }
                    break;
                }
            }//searching for kbin
        }
    }

    for(int i=0;i<totnbins;i++) {
        src_npairs[i] += npairs[i];
        if(need_savg) {
            src_savg[i] += savg[i];
        }
        if(need_weightavg) {
            src_weightavg[i] += weightavg[i];
        }
    }

    return EXIT_SUCCESS;
}
#endif //__SSE4_2__


static inline int countpairs_s_mu_fallback_DOUBLE(const int64_t N0, DOUBLE *x0, DOUBLE *y0, DOUBLE *z0, const weight_struct_DOUBLE *weights0,
                                                  const int64_t N1, DOUBLE *x1, DOUBLE *y1, DOUBLE *z1, const weight_struct_DOUBLE *weights1,
                                                  const int same_cell,
                                                  const int fast_divide,
                                                  const DOUBLE sqr_smax, const DOUBLE sqr_smin, const int nsbin, const int nmu_bins,
                                                  const DOUBLE *supp_sqr, const DOUBLE mu_max, const DOUBLE pimax,
                                                  const DOUBLE off_xwrap, const DOUBLE off_ywrap, const DOUBLE off_zwrap,
                                                  DOUBLE *src_savg, uint64_t *src_npairs,
                                                  DOUBLE *src_weightavg, const weight_method_t weight_method)
{

    (void) fast_divide;
    if(N0 == 0 || N1 == 0) {
        return EXIT_SUCCESS;
    }

    if(src_npairs == NULL) {
        return EXIT_FAILURE;
    }

    /*----------------- FALLBACK CODE --------------------*/
    const int32_t need_savg = src_savg != NULL;
    const int32_t need_weightavg = src_weightavg != NULL;
    const int64_t totnbins = (nmu_bins+1)*(nsbin+1);
    uint64_t npairs[totnbins];
    DOUBLE savg[totnbins], weightavg[totnbins];
    for(int i=0;i<totnbins;i++) {
        npairs[i] = 0;
        if(need_savg) {
            savg[i]=ZERO;
        }
        if(need_weightavg){
            weightavg[i]=ZERO;
        }
    }

    // A copy whose pointers we can advance
    weight_struct_DOUBLE local_w0 = {.weights={NULL}, .num_weights=0},
                         local_w1 = {.weights={NULL}, .num_weights=0};
    pair_struct_DOUBLE pair = {.num_weights=0};
    weight_func_t_DOUBLE weight_func = NULL;
    if(need_weightavg){
        // Same particle list, new copy of num_weights pointers into that list
        local_w0 = *weights0;
        local_w1 = *weights1;

        pair.num_weights = local_w0.num_weights;

        weight_func = get_weight_func_by_method_DOUBLE(weight_method);
    }


    const DOUBLE dmu = mu_max/nmu_bins;
    const DOUBLE inv_dmu = 1.0/dmu;
    const DOUBLE sqr_mu_max = mu_max * mu_max;

    /* naive implementation that is guaranteed to compile */
    int64_t nleft=N1, n_off = 0;
    for(int64_t i=0;i<N0;i++) {
        const DOUBLE xpos = *x0++ + off_xwrap;
        const DOUBLE ypos = *y0++ + off_ywrap;
        const DOUBLE zpos = *z0++ + off_zwrap;
        for(int w = 0; w < pair.num_weights; w++){
            pair.weights0[w].d = *local_w0.weights[w]++;
        }

        /* If in the same cell, unique pairs are guaranteed by not including the current particle */
        if(same_cell == 1) {
            z1++; n_off++;
            nleft--;
        } else {
            /* For a different cell, all pairs are unique pairs, since two cells are only opened for pairs once (accounted for in the assign_ngb_cells function)*/
            while(nleft > 0) {
                /*Particles are sorted on 'z', in increasing order */
                const DOUBLE dz = *z1 - zpos;
                if(dz > -pimax) break;
                z1++; n_off++;
                nleft--;
            }
            /*If no particle in the second cell satisfies distance constraints on 'dz' for the current 'i'th particle in first cell,
              then there can be no more pairs from any particles in the first cell (since the first cell is also sorted in increasing order in 'z')
             */
            if(nleft == 0) {
                i=N0;
                break;
            }
        }
        DOUBLE *localz1 = z1;
        DOUBLE *localx1 = x1 + n_off;
        DOUBLE *localy1 = y1 + n_off;
        for(int w = 0; w < pair.num_weights; w++){
            local_w1.weights[w] = weights1->weights[w] + n_off;
        }

        for(int64_t j=0;j<nleft;j++) {
            const DOUBLE dx = *localx1++ - xpos;
            const DOUBLE dy = *localy1++ - ypos;
            const DOUBLE dz = FABS((*localz1++ - zpos));
            for(int w = 0; w < pair.num_weights; w++){
                pair.weights1[w].d = *local_w1.weights[w]++;
            }

            if(dz >= pimax) break;

            const DOUBLE sqr_dx_dy = dx*dx + dy*dy;
            const DOUBLE sqr_dz = dz*dz;
            const DOUBLE s2 =  sqr_dx_dy + sqr_dz;
            if(s2 >= sqr_smax || s2 < sqr_smin) {
                continue;
            }

            if(sqr_dz >= s2 * sqr_mu_max) {
                continue;
            }
            const DOUBLE mu = SQRT(sqr_dz/s2);

            DOUBLE s, pairweight;
            if(need_savg) {
                s = SQRT(s2);
            }

            if(need_weightavg){
                pair.dx.d = dx;
                pair.dy.d = dy;
                pair.dz.d = dz;
                pairweight = weight_func(&pair);
            }

            int mu_bin = (int) (mu*inv_dmu);
            mu_bin = mu_bin > nmu_bins ? nmu_bins:mu_bin;
            for(int kbin=nsbin-1;kbin>=1;kbin--) {
                if(s2 >= supp_sqr[kbin-1]) {
                    const int ibin = kbin*(nmu_bins+1) + mu_bin;
                    npairs[ibin]++;
                    if(need_savg) {
                        savg[ibin] += s;
                    }
                    if(need_weightavg){
                        weightavg[ibin] += pairweight;
                    }
                    break;
                }
            }
        }
    }
    for(int i=0;i<totnbins;i++) {
        src_npairs[i] += npairs[i];
        if(need_savg) {
            src_savg[i] += savg[i];
        }
        if(need_weightavg){
            src_weightavg[i] += weightavg[i];
        }
    }
   /*----------------- FALLBACK CODE --------------------*/
    return EXIT_SUCCESS;
}
