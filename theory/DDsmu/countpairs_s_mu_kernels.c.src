// # -*- mode: c -*-
/* File: countpairs_s_mu_kernels.c.src */
/*
  This file is a part of the Corrfunc package
  Copyright (C) 2015-- Manodeep Sinha (manodeep@gmail.com)
  License: MIT LICENSE. See LICENSE file under the top-level
  directory at https://github.com/manodeep/Corrfunc/
*/


#include <stdio.h>
#include <stdlib.h>
#include <math.h>
#include <inttypes.h>

#include "function_precision.h"
#include "utils.h"

#include "weight_functions_DOUBLE.h"


#if defined(__AVX__)
#include "avx_calls.h"

static inline int countpairs_s_mu_avx_intrinsics_DOUBLE(const int64_t N0, DOUBLE *x0, DOUBLE *y0, DOUBLE *z0, const weight_struct_DOUBLE *weights0,
                                                        const int64_t N1, DOUBLE *x1, DOUBLE *y1, DOUBLE *z1, const weight_struct_DOUBLE *weights1,
                                                        const int same_cell,
                                                        const unsigned int fast_divide_and_NR_steps,
                                                        const DOUBLE sqr_smax, const DOUBLE sqr_smin, const int nsbin,
                                                        const int nmu_bins, const DOUBLE *supp_sqr, const DOUBLE mu_max, const DOUBLE pimax,
                                                        const DOUBLE off_xwrap, const DOUBLE off_ywrap, const DOUBLE off_zwrap,
                                                        DOUBLE *src_savg, uint64_t *src_npairs,
                                                        DOUBLE *src_weightavg, const weight_method_t weight_method)
{
    if(N0 == 0 || N1 == 0) {
        return EXIT_SUCCESS;
    }

    if(src_npairs == NULL) {
        return EXIT_FAILURE;
    }

    const int32_t need_savg = src_savg != NULL;
    const int32_t need_weightavg = src_weightavg != NULL;

    const int64_t totnbins = (nmu_bins+1)*(nsbin+1);
    uint64_t npairs[totnbins];
    DOUBLE savg[totnbins], weightavg[totnbins];
    for(int64_t i=0;i<totnbins;i++) {
        npairs[i] = 0;
        if(need_savg) {
            savg[i] = ZERO;
        }
        if(need_weightavg){
            weightavg[i] = ZERO;
        }
    }


    AVX_FLOATS m_supp_sqr[nsbin];
    AVX_FLOATS m_kbin[nsbin];
    for(int i=0;i<nsbin;i++) {
        m_supp_sqr[i] = AVX_SET_FLOAT(supp_sqr[i]);
        m_kbin[i] = AVX_SET_FLOAT((DOUBLE) i);
    }

    /* const AVX_FLOATS m_mumax = AVX_SET_FLOAT(mu_max); */
    const DOUBLE sqr_mumax = mu_max*mu_max;
    const DOUBLE dmu = mu_max/(DOUBLE) nmu_bins;
    const DOUBLE inv_dmu = 1.0/dmu;

    // A copy whose pointers we can advance
    weight_struct_DOUBLE local_w0 = {.weights={NULL}, .num_weights=0},
                         local_w1 = {.weights={NULL}, .num_weights=0};
    pair_struct_DOUBLE pair = {.num_weights=0};
    avx_weight_func_t_DOUBLE avx_weight_func = NULL;
    weight_func_t_DOUBLE fallback_weight_func = NULL;
    if(need_weightavg){
        // Same particle list, new copy of num_weights pointers into that list
        local_w0 = *weights0;
        local_w1 = *weights1;

        pair.num_weights = local_w0.num_weights;

        avx_weight_func = get_avx_weight_func_by_method_DOUBLE(weight_method);
        fallback_weight_func = get_weight_func_by_method_DOUBLE(weight_method);
    }

    int64_t prev_j = 0, n_off = 0;
    for(int64_t i=0;i<N0;i++) {
        const DOUBLE xpos = *x0++ + off_xwrap;
        const DOUBLE ypos = *y0++ + off_ywrap;
        const DOUBLE zpos = *z0++ + off_zwrap;
        for(int w = 0; w < pair.num_weights; w++){
            // local_w0.weights[w] is a pointer to a float in the particle list of weights,
            // just as x0 is a pointer into the list of x-positions.
            // The advancement of the local_w0.weights[w] pointer should always mirror x0.
            pair.weights0[w].a = AVX_SET_FLOAT(*(local_w0.weights[w])++);
        }

        int64_t j;
        if(same_cell == 1) {
            z1++; n_off++;
            j = i+1;
        } else {
            for(;prev_j<N1;prev_j++) {
                const DOUBLE dz = *z1 - zpos;
                if(dz > -pimax) break;
                z1++; n_off++;
            }
            if(prev_j == N1) {
                i = N0;
                break;
            }
            j = prev_j;
        }
        DOUBLE *localz1 = z1;
        DOUBLE *localx1 = x1 + n_off;
        DOUBLE *localy1 = y1 + n_off;
        for(int w = 0; w < local_w1.num_weights; w++){
            local_w1.weights[w] = weights1->weights[w] + n_off;
        }

        for(;j<=(N1 - AVX_NVEC);j+=AVX_NVEC) {
            const AVX_FLOATS m_xpos    = AVX_SET_FLOAT(xpos);
            const AVX_FLOATS m_ypos    = AVX_SET_FLOAT(ypos);
            const AVX_FLOATS m_zpos    = AVX_SET_FLOAT(zpos);

            union int8 {
                AVX_INTS m_ibin;
                int ibin[AVX_NVEC];
            };
            union int8 union_finalbin;
            union float8{
                AVX_FLOATS m_Dperp;
                DOUBLE Dperp[AVX_NVEC];
            };
            union float8 union_mDperp;


            const AVX_FLOATS m_x1 = AVX_LOAD_FLOATS_UNALIGNED(localx1);
            const AVX_FLOATS m_y1 = AVX_LOAD_FLOATS_UNALIGNED(localy1);
            const AVX_FLOATS m_z1 = AVX_LOAD_FLOATS_UNALIGNED(localz1);

            localx1 += AVX_NVEC;//this might actually exceed the allocated range but we will never dereference that
            localy1 += AVX_NVEC;
            localz1 += AVX_NVEC;

            for(int w = 0; w < pair.num_weights; w++){
                pair.weights1[w].a = AVX_LOAD_FLOATS_UNALIGNED(local_w1.weights[w]);
                local_w1.weights[w] += AVX_NVEC;
            }

            union float8_weights{
                AVX_FLOATS m_weights;
                DOUBLE weights[NVEC];
            };
            union float8_weights union_mweight;

            const AVX_FLOATS m_pimax = AVX_SET_FLOAT((DOUBLE) pimax);
            const AVX_FLOATS m_sqr_smax = m_supp_sqr[nsbin-1];
            const AVX_FLOATS m_sqr_smin = m_supp_sqr[0];
            const AVX_FLOATS m_inv_dmu    = AVX_SET_FLOAT(inv_dmu);
            const AVX_FLOATS m_sqr_mumax = AVX_SET_FLOAT(sqr_mumax);

            const AVX_FLOATS m_zero = AVX_SET_FLOAT(ZERO);
            const AVX_FLOATS m_nmu_bins     = AVX_SET_FLOAT((DOUBLE) nmu_bins);
            const AVX_FLOATS m_one    = AVX_SET_FLOAT((DOUBLE) 1);

            const AVX_FLOATS m_xdiff = AVX_SUBTRACT_FLOATS(m_x1, m_xpos);  //(x[j] - x0)
            const AVX_FLOATS m_ydiff = AVX_SUBTRACT_FLOATS(m_y1, m_ypos);  //(y[j] - y0)
            AVX_FLOATS m_zdiff = AVX_SUBTRACT_FLOATS(m_z1, m_zpos);  //z2[j:j+NVEC-1] - z1

            const AVX_FLOATS m_sqr_xdiff = AVX_SQUARE_FLOAT(m_xdiff);  //(x0 - x[j])^2
            const AVX_FLOATS m_sqr_ydiff = AVX_SQUARE_FLOAT(m_ydiff);  //(y0 - y[j])^2
            const AVX_FLOATS m_sqr_zdiff = AVX_SQUARE_FLOAT(m_zdiff);  //(z0 - z[j])^2

            AVX_FLOATS s2  = AVX_ADD_FLOATS(m_sqr_zdiff, AVX_ADD_FLOATS(m_sqr_xdiff, m_sqr_ydiff));//s^2 = dz^2 + dx^2 + dy^2
            m_zdiff = AVX_MAX_FLOATS(m_zdiff,AVX_SUBTRACT_FLOATS(m_zero,m_zdiff));//dz = fabs(dz) => dz = max(dz, -dz);

            AVX_FLOATS m_mask_left;
            AVX_FLOATS max_sqr_dz = AVX_MULTIPLY_FLOATS(s2, m_sqr_mumax);

            //Do all the distance cuts using masks here in new scope
            {
                //the z2 arrays are sorted in increasing order. which means
                //the z2 value will increase in any future iteration of j.
                //that implies the zdiff values are also monotonically increasing
                //Therefore, if none of the zdiff values are less than pimax, then
                //no future iteration in j can produce a zdiff value less than pimax.
                AVX_FLOATS m_mask_pimax = AVX_COMPARE_FLOATS(m_zdiff,m_pimax,_CMP_LT_OS);
                if(AVX_TEST_COMPARISON(m_mask_pimax) == 0) {
                    j=N1;
                    break;
                }

                const AVX_FLOATS m_mu_mask = AVX_COMPARE_FLOATS(m_sqr_zdiff, max_sqr_dz, _CMP_LT_OS);
                const AVX_FLOATS m_smax_mask = AVX_COMPARE_FLOATS(s2, m_sqr_smax, _CMP_LT_OS);//check for s2 < sqr_smax
                const AVX_FLOATS m_smin_mask = AVX_COMPARE_FLOATS(s2, m_sqr_smin, _CMP_GE_OS);//check for s2 >= sqr_smin
                const AVX_FLOATS m_s2_mask = AVX_BITWISE_AND(m_smax_mask,m_smin_mask);

                //Create a combined mask by bitwise and of m1 and m_mask_left.
                //This gives us the mask for all sqr_smin <= s2 < sqr_smax
                // + mu_min <= mu < mu_max
                m_mask_left = AVX_BITWISE_AND(m_mu_mask, m_s2_mask);

                //If not, continue with the next iteration of j-loop
                if(AVX_TEST_COMPARISON(m_mask_left) == 0) {
                    continue;
                }

            }

            //There is some s2 that satisfies sqr_smin <= s2 < sqr_smax && mu_min <= |dz| < mu_max
            s2 = AVX_BLEND_FLOATS_WITH_MASK(m_sqr_smax, s2, m_mask_left);
            /*m_sqr_mu := dz^2/s^2 (with masked elements set to mu_max */
            AVX_FLOATS m_sqr_mu = AVX_SETZERO_FLOAT();


            /* Check if fast_divide is enabled and either use the normal divide or
               use the approx. reciprocal followed by `fast_divide_and_NR_steps` number
               of Newton-Raphson steps to improve numerical accuracy.
               
               macro is defined in `avx_calls.h`
            */
            CHECK_AND_FAST_DIVIDE(m_sqr_mu, m_sqr_zdiff, s2, fast_divide_and_NR_steps);
                
            const AVX_FLOATS m_mu = AVX_SQRT_FLOAT(AVX_BLEND_FLOATS_WITH_MASK(m_sqr_mumax, m_sqr_mu, m_mask_left));
            
            if(need_savg) {
                union_mDperp.m_Dperp = AVX_SQRT_FLOAT(s2);
            }
            if(need_weightavg){
                pair.dx.a = m_xdiff;
                pair.dy.a = m_ydiff;
                pair.dz.a = m_zdiff;

                union_mweight.m_weights = avx_weight_func(&pair);
            }

            const AVX_FLOATS m_mubin = AVX_MULTIPLY_FLOATS(m_mu,m_inv_dmu);
            AVX_FLOATS m_sbin     = AVX_SET_FLOAT((DOUBLE) 0);
            //AVX_FLOATS m_all_ones  = AVX_CAST_INT_TO_FLOAT(AVX_SET_INT(-1));
            for(int kbin=nsbin-1;kbin>=1;kbin--) {
                const AVX_FLOATS m_mask_low = AVX_COMPARE_FLOATS(s2,m_supp_sqr[kbin-1],_CMP_GE_OS);
                const AVX_FLOATS m_bin_mask = AVX_BITWISE_AND(m_mask_low,m_mask_left);
                m_sbin = AVX_BLEND_FLOATS_WITH_MASK(m_sbin,m_kbin[kbin], m_bin_mask);
                m_mask_left = AVX_COMPARE_FLOATS(s2, m_supp_sqr[kbin-1],_CMP_LT_OS);
                //m_mask_left = AVX_XOR_FLOATS(m_mask_low, m_all_ones);//XOR with 0xFFFF... gives the bins that are smaller than m_supp_sqr[kbin] (and is faster than cmp_p(s/d) in theory)
                const int test = AVX_TEST_COMPARISON(m_mask_left);
                if(test==0) {
                    break;
                }
            }
            const AVX_FLOATS m_nmu_bins_p1 = AVX_ADD_FLOATS(m_nmu_bins,m_one);
            const AVX_FLOATS m_binproduct = AVX_ADD_FLOATS(AVX_MULTIPLY_FLOATS(m_sbin,m_nmu_bins_p1),m_mubin);
            union_finalbin.m_ibin = AVX_TRUNCATE_FLOAT_TO_INT(m_binproduct);

            //update the histograms
#if defined(__ICC) || defined(__INTEL_COMPILER)
#pragma unroll(AVX_NVEC)
#endif
            for(int jj=0;jj<AVX_NVEC;jj++) {
                int ibin = union_finalbin.ibin[jj];
                npairs[ibin]++;
                if(need_savg) {
                    savg[ibin] += union_mDperp.Dperp[jj];
                }
                if(need_weightavg){
                    const DOUBLE weight = union_mweight.weights[jj];
                    weightavg[ibin] += weight;
                }
            }
        }


        //remainder loop
        for(;j<N1;j++){
            const DOUBLE dz = FABS(*localz1++ - zpos);
            const DOUBLE dx = *localx1++ - xpos;
            const DOUBLE dy = *localy1++ - ypos;
            for(int w = 0; w < pair.num_weights; w++){
                pair.weights1[w].d = *local_w1.weights[w]++;
            }

            if(dz >= pimax) {
                break;
            }

            const DOUBLE sqr_dx_dy = dx*dx + dy*dy;
            const DOUBLE sqr_dz = dz*dz;
            const DOUBLE s2 =  sqr_dx_dy + sqr_dz;
            if(s2 >= sqr_smax || s2 < sqr_smin)
                continue;
            if(sqr_dz >= s2 * sqr_mumax) continue;
            const DOUBLE mu = SQRT(sqr_dz/s2);

            DOUBLE s, pairweight;
            if(need_savg) {
                s = SQRT(s2);
            }
            if(need_weightavg){
                pair.dx.d = dx;
                pair.dy.d = dy;
                pair.dz.d = dz;
                pairweight = fallback_weight_func(&pair);
            }

            int mu_bin = (int) (mu*inv_dmu);
            mu_bin = mu_bin > nmu_bins ? nmu_bins:mu_bin;
            for(int kbin=nsbin-1;kbin>=1;kbin--) {
                if(s2 >= supp_sqr[kbin-1]) {
                    const int ibin = kbin*(nmu_bins+1) + mu_bin;
                    npairs[ibin]++;
                    if(need_savg) {
                        savg[ibin] += s;
                    }
                    if(need_weightavg){
                        weightavg[ibin] += pairweight;
                    }
                    break;
                }
            }
        }//remainder loop over second set of particles
    }//loop over first set of particles

	for(int i=0;i<totnbins;i++) {
		src_npairs[i] += npairs[i];
        if(need_savg) {
            src_savg[i] += savg[i];
        }
        if(need_weightavg) {
            src_weightavg[i] += weightavg[i];
        }
    }

    return EXIT_SUCCESS;
}
#endif //__AVX__



#if defined (__SSE4_2__)
#include "sse_calls.h"

static inline int countpairs_s_mu_sse_intrinsics_DOUBLE(const int64_t N0, DOUBLE *x0, DOUBLE *y0, DOUBLE *z0, const weight_struct_DOUBLE *weights0,
                                                        const int64_t N1, DOUBLE *x1, DOUBLE *y1, DOUBLE *z1, const weight_struct_DOUBLE *weights1,
                                                        const int same_cell,
                                                        const unsigned int fast_divide_and_NR_steps,
                                                        const DOUBLE sqr_smax, const DOUBLE sqr_smin, const int nsbin, const int nmu_bins,
                                                        const DOUBLE *supp_sqr, const DOUBLE mu_max, const DOUBLE pimax,
                                                        const DOUBLE off_xwrap, const DOUBLE off_ywrap, const DOUBLE off_zwrap,
                                                        DOUBLE *src_savg, uint64_t *src_npairs,
                                                        DOUBLE *src_weightavg, const weight_method_t weight_method)
{
    (void) fast_divide_and_NR_steps;
    
    if(N0 == 0 || N1 == 0) {
        return EXIT_SUCCESS;
    }

    if(src_npairs == NULL) {
        return EXIT_FAILURE;
    }

    const int32_t need_savg = src_savg != NULL;
    const int32_t need_weightavg = src_weightavg != NULL;
    const int64_t totnbins = (nmu_bins+1) * (nsbin+1);
    uint64_t npairs[totnbins];
    DOUBLE savg[totnbins], weightavg[totnbins];
    for(int64_t i=0;i<totnbins;i++) {
        npairs[i] = 0;
        if (need_savg) {
            savg[i] = ZERO;
        }
        if(need_weightavg){
            weightavg[i] = ZERO;
        }
    }

    SSE_FLOATS m_kbin[nsbin];
    SSE_FLOATS m_supp_sqr[nsbin];
    for(int i=0;i<nsbin;i++) {
        m_kbin[i] = SSE_SET_FLOAT((DOUBLE) i);
        m_supp_sqr[i] = SSE_SET_FLOAT(supp_sqr[i]);
    }

    const DOUBLE sqr_mumax = mu_max*mu_max;
    const DOUBLE dmu = mu_max/(DOUBLE) nmu_bins;
    const DOUBLE inv_dmu = 1.0/dmu;


    // A copy whose pointers we can advance
    weight_struct_DOUBLE local_w0 = {.weights={NULL}, .num_weights=0},
                         local_w1 = {.weights={NULL}, .num_weights=0};
    pair_struct_DOUBLE pair = {.num_weights=0};
    sse_weight_func_t_DOUBLE sse_weight_func = NULL;
    weight_func_t_DOUBLE fallback_weight_func = NULL;
    if(need_weightavg){
      // Same particle list, new copy of num_weights pointers into that list
      local_w0 = *weights0;
      local_w1 = *weights1;

      pair.num_weights = local_w0.num_weights;

      sse_weight_func = get_sse_weight_func_by_method_DOUBLE(weight_method);
      fallback_weight_func = get_weight_func_by_method_DOUBLE(weight_method);
    }

    int64_t prev_j = 0, n_off = 0;
    for(int64_t i=0;i<N0;i++) {
        const DOUBLE xpos = *x0++ + off_xwrap;
        const DOUBLE ypos = *y0++ + off_ywrap;
        const DOUBLE zpos = *z0++ + off_zwrap;
        for(int w = 0; w < pair.num_weights; w++){
            // local_w0.weights[w] is a pointer to a float in the particle list of weights,
            // just as x0 is a pointer into the list of x-positions.
            // The advancement of the local_w0.weights[w] pointer should always mirror x0.
            pair.weights0[w].s = SSE_SET_FLOAT(*local_w0.weights[w]++);
        }

        int64_t j;
        if(same_cell == 1) {
            z1++; n_off++;
            j = i+1;
        } else {
            for(;prev_j<N1;prev_j++) {
                const DOUBLE dz = *z1 - zpos;
                if(dz > -pimax) break;
                z1++; n_off++;
            }
            if(prev_j == N1) {
                i = N0;
                break;
            }
            j = prev_j;
        }
        DOUBLE *localz1 = z1;
        DOUBLE *localx1 = x1 + n_off;
        DOUBLE *localy1 = y1 + n_off;
        for(int w = 0; w < local_w1.num_weights; w++){
            local_w1.weights[w] = weights1->weights[w] + n_off;
        }

        for(;j<=(N1 - SSE_NVEC);j+=SSE_NVEC){

            union int4{
                SSE_INTS m_ibin;
                int ibin[SSE_NVEC];
            };
            union int4 union_finalbin;

            union float4{
                SSE_FLOATS m_Dperp;
                DOUBLE Dperp[SSE_NVEC];
            };
            union float4 union_mDperp;

            const SSE_FLOATS m_xpos = SSE_SET_FLOAT(xpos);
            const SSE_FLOATS m_ypos = SSE_SET_FLOAT(ypos);
            const SSE_FLOATS m_zpos = SSE_SET_FLOAT(zpos);

            const SSE_FLOATS m_x1 = SSE_LOAD_FLOATS_UNALIGNED(localx1);
            const SSE_FLOATS m_y1 = SSE_LOAD_FLOATS_UNALIGNED(localy1);
            const SSE_FLOATS m_z1 = SSE_LOAD_FLOATS_UNALIGNED(localz1);

            localx1 += SSE_NVEC;
            localy1 += SSE_NVEC;
            localz1 += SSE_NVEC;

            for(int w = 0; w < pair.num_weights; w++){
                pair.weights1[w].s = SSE_LOAD_FLOATS_UNALIGNED(local_w1.weights[w]);
                local_w1.weights[w] += SSE_NVEC;
            }

            union float4_weights{
                SSE_FLOATS m_weights;
                DOUBLE weights[SSE_NVEC];
            };
            union float4_weights union_mweight;

            const SSE_FLOATS m_pimax = SSE_SET_FLOAT((DOUBLE) pimax);
            const SSE_FLOATS m_sqr_smax = m_supp_sqr[nsbin-1];
            const SSE_FLOATS m_sqr_smin = m_supp_sqr[0];
            const SSE_FLOATS m_sqr_mumax = SSE_SET_FLOAT(sqr_mumax);
            const SSE_FLOATS m_inv_dmu    = SSE_SET_FLOAT(inv_dmu);
            const SSE_FLOATS m_zero = SSE_SET_FLOAT(ZERO);
            const SSE_FLOATS m_nmu_bins     = SSE_SET_FLOAT((DOUBLE) nmu_bins);
            const SSE_FLOATS m_one    = SSE_SET_FLOAT((DOUBLE) 1);

            const SSE_FLOATS m_xdiff = SSE_SUBTRACT_FLOATS(m_x1, m_xpos);  //(x[j] - x0)
            const SSE_FLOATS m_ydiff = SSE_SUBTRACT_FLOATS(m_y1, m_ypos);  //(y[j] - y0)
            SSE_FLOATS m_zdiff = SSE_SUBTRACT_FLOATS(m_z1, m_zpos);  //z2[j:j+NVEC-1] - z1

            const SSE_FLOATS m_sqr_xdiff = SSE_SQUARE_FLOAT(m_xdiff);
            const SSE_FLOATS m_sqr_ydiff = SSE_SQUARE_FLOAT(m_ydiff);
            const SSE_FLOATS m_sqr_zdiff = SSE_SQUARE_FLOAT(m_zdiff);

            SSE_FLOATS s2  = SSE_ADD_FLOATS(m_sqr_zdiff, SSE_ADD_FLOATS(m_sqr_xdiff, m_sqr_ydiff));//s^2 = dx^2 + dy^2 + dz^2
            m_zdiff = SSE_MAX_FLOATS(m_zdiff,SSE_SUBTRACT_FLOATS(m_zero,m_zdiff));//dz = fabs(dz) => dz = max(dz, -dz);

            SSE_FLOATS m_mask_left;
            SSE_FLOATS max_sqr_dz = SSE_MULTIPLY_FLOATS(s2, m_sqr_mumax);

            //Do all the distance cuts using masks here in new scope
            {
                //the z2 arrays are sorted in increasing order. which means
                //the z2 value will increase in any future iteration of j.
                //that implies the zdiff values are also monotonically increasing
                //Therefore, if none of the zdiff values are less than pimax, then
                //no future iteration in j can produce a zdiff value less than pimax.
                SSE_FLOATS m_mask_pimax = SSE_COMPARE_FLOATS_LT(m_zdiff,m_pimax);
                if(SSE_TEST_COMPARISON(m_mask_pimax) == 0) {
                    j=N1;
                    break;
                }

                const SSE_FLOATS m_mu_mask = SSE_COMPARE_FLOATS_LT(m_sqr_zdiff, max_sqr_dz);
                const SSE_FLOATS m_smax_mask = SSE_COMPARE_FLOATS_LT(s2, m_sqr_smax);
                const SSE_FLOATS m_smin_mask = SSE_COMPARE_FLOATS_GE(s2, m_sqr_smin);
                const SSE_FLOATS m_s2_mask = SSE_BITWISE_AND(m_smax_mask,m_smin_mask);

                //Create a combined mask by bitwise and of m1 and m_mask_left.
                //This gives us the mask for all sqr_smin <= s2 < sqr_smax
                // + mu_min <= mu < mu_max
                m_mask_left = SSE_BITWISE_AND(m_mu_mask, m_s2_mask);

                //If not, continue with the next iteration of j-loop
                if(SSE_TEST_COMPARISON(m_mask_left) == 0) {
                    continue;
                }

            }

            //There is some s2 that satisfies sqr_smin <= s2 < sqr_smax && mu_min <= |dz| < mu_max
            s2 = SSE_BLEND_FLOATS_WITH_MASK(m_sqr_smax, s2, m_mask_left);
            const SSE_FLOATS m_mu = SSE_SQRT_FLOAT(SSE_BLEND_FLOATS_WITH_MASK(m_sqr_mumax, SSE_DIVIDE_FLOATS(m_sqr_zdiff, s2), m_mask_left));

            if(need_savg) {
                union_mDperp.m_Dperp = SSE_SQRT_FLOAT(s2);
            }
            if(need_weightavg){
                pair.dx.s = m_xdiff;
                pair.dy.s = m_ydiff;
                pair.dz.s = m_zdiff;

                union_mweight.m_weights = sse_weight_func(&pair);
            }

            const SSE_FLOATS m_mubin = SSE_MULTIPLY_FLOATS(m_mu,m_inv_dmu);
            SSE_FLOATS m_sbin     = SSE_SET_FLOAT((DOUBLE) 0);
            //SSE_FLOATS m_all_ones  = SSE_CAST_INT_TO_FLOAT(SSE_SET_INT(-1));
            for(int kbin=nsbin-1;kbin>=1;kbin--) {
                const SSE_FLOATS m_mask_low = SSE_COMPARE_FLOATS_GE(s2,m_supp_sqr[kbin-1]);
                const SSE_FLOATS m_bin_mask = SSE_BITWISE_AND(m_mask_low,m_mask_left);
                m_sbin = SSE_BLEND_FLOATS_WITH_MASK(m_sbin,m_kbin[kbin], m_bin_mask);
                m_mask_left = SSE_COMPARE_FLOATS_LT(s2, m_supp_sqr[kbin-1]);
                //XOR with 0xFFFF... gives the bins that are smaller than m_supp_sqr[kbin] (and is faster than cmp_p(s/d) in theory)
                //m_mask_left = SSE_XOR_FLOATS(m_mask_low, m_all_ones);
                const int test = SSE_TEST_COMPARISON(m_mask_left);
                if(test==0) {
                    break;
                }
            }
            const SSE_FLOATS m_nmu_bins_p1 = SSE_ADD_FLOATS(m_nmu_bins,m_one);
            const SSE_FLOATS m_binproduct = SSE_ADD_FLOATS(SSE_MULTIPLY_FLOATS(m_sbin,m_nmu_bins_p1),m_mubin);
            union_finalbin.m_ibin = SSE_TRUNCATE_FLOAT_TO_INT(m_binproduct);

            //update the histograms
#if defined(__ICC) || defined(__INTEL_COMPILER)
#pragma unroll(SSE_NVEC)
#endif
            for(int jj=0;jj<SSE_NVEC;jj++) {
                int ibin = union_finalbin.ibin[jj];
                npairs[ibin]++;
                if(need_savg) {
                    savg[ibin] += union_mDperp.Dperp[jj];
                }
                if(need_weightavg){
                    const DOUBLE weight = union_mweight.weights[jj];
                    weightavg[ibin] += weight;
                }
            }
        }


        for(;j<N1;j++) {
            const DOUBLE dx = *localx1++ - xpos;
            const DOUBLE dy = *localy1++ - ypos;
            const DOUBLE dz = FABS(*localz1++ - zpos);
            for(int w = 0; w < pair.num_weights; w++){
                pair.weights1[w].d = *local_w1.weights[w]++;
            }

            if(dz >= pimax) break;

            const DOUBLE sqr_dx_dy = dx*dx + dy*dy;
            const DOUBLE sqr_dz = dz*dz;
            const DOUBLE s2 =  sqr_dx_dy + sqr_dz;
            if(s2 >= sqr_smax || s2 < sqr_smin)
                continue;
            if(sqr_dz >= s2 * sqr_mumax) continue;
            const DOUBLE mu = SQRT(sqr_dz/s2);

            DOUBLE s, pairweight;            
            if(need_weightavg){
                pair.dx.d = dx;
                pair.dy.d = dy;
                pair.dz.d = dz;
                pairweight = fallback_weight_func(&pair);                
            }

            if(need_savg) {
                s = SQRT(s2);
            }

            int mu_bin = (int) (mu*inv_dmu);
            mu_bin = mu_bin > nmu_bins ? nmu_bins:mu_bin;
            for(int kbin=nsbin-1;kbin>=1;kbin--) {
                if(s2 >= supp_sqr[kbin-1]) {
                    const int ibin = kbin*(nmu_bins+1) + mu_bin;
                    npairs[ibin]++;
                    if(need_savg) {
                        savg[ibin] += s;
                    }
                    if(need_weightavg){
                        weightavg[ibin] += pairweight;
                    }
                    break;
                }
            }//searching for kbin
        }
    }

    for(int i=0;i<totnbins;i++) {
        src_npairs[i] += npairs[i];
        if(need_savg) {
            src_savg[i] += savg[i];
        }
        if(need_weightavg) {
            src_weightavg[i] += weightavg[i];
        }
    }

    return EXIT_SUCCESS;
}
#endif //__SSE4_2__


static inline int countpairs_s_mu_fallback_DOUBLE(const int64_t N0, DOUBLE *x0, DOUBLE *y0, DOUBLE *z0, const weight_struct_DOUBLE *weights0,
                                                  const int64_t N1, DOUBLE *x1, DOUBLE *y1, DOUBLE *z1, const weight_struct_DOUBLE *weights1,
                                                  const int same_cell,
                                                  const unsigned int fast_divide_and_NR_steps,
                                                  const DOUBLE sqr_smax, const DOUBLE sqr_smin, const int nsbin, const int nmu_bins,
                                                  const DOUBLE *supp_sqr, const DOUBLE mu_max, const DOUBLE pimax,
                                                  const DOUBLE off_xwrap, const DOUBLE off_ywrap, const DOUBLE off_zwrap,
                                                  DOUBLE *src_savg, uint64_t *src_npairs,
                                                  DOUBLE *src_weightavg, const weight_method_t weight_method)
{

    (void) fast_divide_and_NR_steps;
    if(N0 == 0 || N1 == 0) {
        return EXIT_SUCCESS;
    }

    if(src_npairs == NULL) {
        return EXIT_FAILURE;
    }

    /*----------------- FALLBACK CODE --------------------*/
    const int32_t need_savg = src_savg != NULL;
    const int32_t need_weightavg = src_weightavg != NULL;
    const int64_t totnbins = (nmu_bins+1)*(nsbin+1);
    uint64_t npairs[totnbins];
    DOUBLE savg[totnbins], weightavg[totnbins];
    for(int i=0;i<totnbins;i++) {
        npairs[i] = 0;
        if(need_savg) {
            savg[i]=ZERO;
        }
        if(need_weightavg){
            weightavg[i]=ZERO;
        }
    }

    // A copy whose pointers we can advance
    weight_struct_DOUBLE local_w0 = {.weights={NULL}, .num_weights=0},
                         local_w1 = {.weights={NULL}, .num_weights=0};
    pair_struct_DOUBLE pair = {.num_weights=0};
    weight_func_t_DOUBLE weight_func = NULL;
    if(need_weightavg){
        // Same particle list, new copy of num_weights pointers into that list
        local_w0 = *weights0;
        local_w1 = *weights1;

        pair.num_weights = local_w0.num_weights;

        weight_func = get_weight_func_by_method_DOUBLE(weight_method);
    }


    const DOUBLE dmu = mu_max/nmu_bins;
    const DOUBLE inv_dmu = 1.0/dmu;
    const DOUBLE sqr_mu_max = mu_max * mu_max;

    /* naive implementation that is guaranteed to compile */
    int64_t nleft=N1, n_off = 0;
    for(int64_t i=0;i<N0;i++) {
        const DOUBLE xpos = *x0++ + off_xwrap;
        const DOUBLE ypos = *y0++ + off_ywrap;
        const DOUBLE zpos = *z0++ + off_zwrap;
        for(int w = 0; w < pair.num_weights; w++){
            pair.weights0[w].d = *local_w0.weights[w]++;
        }

        /* If in the same cell, unique pairs are guaranteed by not including the current particle */
        if(same_cell == 1) {
            z1++; n_off++;
            nleft--;
        } else {
            /* For a different cell, all pairs are unique pairs, since two cells are only opened for pairs once (accounted for in the assign_ngb_cells function)*/
            while(nleft > 0) {
                /*Particles are sorted on 'z', in increasing order */
                const DOUBLE dz = *z1 - zpos;
                if(dz > -pimax) break;
                z1++; n_off++;
                nleft--;
            }
            /*If no particle in the second cell satisfies distance constraints on 'dz' for the current 'i'th particle in first cell,
              then there can be no more pairs from any particles in the first cell (since the first cell is also sorted in increasing order in 'z')
             */
            if(nleft == 0) {
                i=N0;
                break;
            }
        }
        DOUBLE *localz1 = z1;
        DOUBLE *localx1 = x1 + n_off;
        DOUBLE *localy1 = y1 + n_off;
        for(int w = 0; w < pair.num_weights; w++){
            local_w1.weights[w] = weights1->weights[w] + n_off;
        }

        for(int64_t j=0;j<nleft;j++) {
            const DOUBLE dx = *localx1++ - xpos;
            const DOUBLE dy = *localy1++ - ypos;
            const DOUBLE dz = FABS((*localz1++ - zpos));
            for(int w = 0; w < pair.num_weights; w++){
                pair.weights1[w].d = *local_w1.weights[w]++;
            }

            if(dz >= pimax) break;

            const DOUBLE sqr_dx_dy = dx*dx + dy*dy;
            const DOUBLE sqr_dz = dz*dz;
            const DOUBLE s2 =  sqr_dx_dy + sqr_dz;
            if(s2 >= sqr_smax || s2 < sqr_smin) {
                continue;
            }

            if(sqr_dz >= s2 * sqr_mu_max) {
                continue;
            }
            const DOUBLE mu = SQRT(sqr_dz/s2);

            DOUBLE s, pairweight;
            if(need_savg) {
                s = SQRT(s2);
            }

            if(need_weightavg){
                pair.dx.d = dx;
                pair.dy.d = dy;
                pair.dz.d = dz;
                pairweight = weight_func(&pair);
            }

            int mu_bin = (int) (mu*inv_dmu);
            mu_bin = mu_bin > nmu_bins ? nmu_bins:mu_bin;
            for(int kbin=nsbin-1;kbin>=1;kbin--) {
                if(s2 >= supp_sqr[kbin-1]) {
                    const int ibin = kbin*(nmu_bins+1) + mu_bin;
                    npairs[ibin]++;
                    if(need_savg) {
                        savg[ibin] += s;
                    }
                    if(need_weightavg){
                        weightavg[ibin] += pairweight;
                    }
                    break;
                }
            }
        }
    }
    for(int i=0;i<totnbins;i++) {
        src_npairs[i] += npairs[i];
        if(need_savg) {
            src_savg[i] += savg[i];
        }
        if(need_weightavg){
            src_weightavg[i] += weightavg[i];
        }
    }
   /*----------------- FALLBACK CODE --------------------*/
    return EXIT_SUCCESS;
}
