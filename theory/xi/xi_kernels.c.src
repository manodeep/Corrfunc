// # -*- mode: c -*-
/* File: xi_kernels.c.src */
/*
  This file is a part of the Corrfunc package
  Copyright (C) 2015-- Manodeep Sinha (manodeep@gmail.com)
  License: MIT LICENSE. See LICENSE file under the top-level
  directory at https://github.com/manodeep/Corrfunc/
*/


#include <stdio.h>
#include <stdlib.h>
#include <math.h>
#include <stdint.h>

#include "function_precision.h"
#include "utils.h"

#include "weight_functions_DOUBLE.h"

#if defined(__AVX__)
#include "avx_calls.h"

static inline int xi_avx_intrinsics_DOUBLE(DOUBLE *x1, DOUBLE *y1, DOUBLE *z1, const weight_struct_DOUBLE *weights1, const int64_t N1,
                                           DOUBLE *x2, DOUBLE *y2, DOUBLE *z2, const weight_struct_DOUBLE *weights2, const int64_t N2, const int same_cell,
                                           const DOUBLE sqr_rmax, const DOUBLE sqr_rmin, const int nbin, const DOUBLE *rupp_sqr, const DOUBLE rmax,
                                           const DOUBLE off_xwrap, const DOUBLE off_ywrap, const DOUBLE off_zwrap
                                           ,DOUBLE *src_ravg
                                           ,uint64_t *src_npairs,
                                           DOUBLE *src_weightavg, const weight_method_t weight_method)
{
    uint64_t npair[nbin];
    AVX_FLOATS m_rupp_sqr[nbin];
    for(int i=0;i<nbin;i++) {
        npair[i] = 0;
        m_rupp_sqr[i] = AVX_SET_FLOAT(rupp_sqr[i]);
    }

    const int32_t need_ravg = src_ravg != NULL;
    const int32_t need_weightavg = src_weightavg != NULL;
    
    AVX_FLOATS m_kbin[nbin];
    DOUBLE ravg[nbin], weightavg[nbin];
    if(need_ravg || need_weightavg){
      for(int i=0;i<nbin;i++) {
        m_kbin[i] = AVX_SET_FLOAT((DOUBLE) i);
        if(need_ravg){
          ravg[i] = ZERO;
        }
        if(need_weightavg){
          weightavg[i] = ZERO;
        }
      }
    }
    
    // A copy whose pointers we can advance
    weight_struct_DOUBLE local_w1 = {.weights={NULL}, .num_weights=0}, 
                       local_w2 = {.weights={NULL}, .num_weights=0};
    pair_struct_DOUBLE pair = {.num_weights=0};
    avx_weight_func_t_DOUBLE avx_weight_func = NULL;
    weight_func_t_DOUBLE fallback_weight_func = NULL;
    if(need_weightavg){
      // Same particle list, new copy of num_weights pointers into that list
      local_w1 = *weights1;
      local_w2 = *weights2;
      
      pair.num_weights = local_w1.num_weights;
      
      avx_weight_func = get_avx_weight_func_by_method_DOUBLE(weight_method);
      fallback_weight_func = get_weight_func_by_method_DOUBLE(weight_method);
    }

    int64_t prev_j=0, n_off=0;
    for(int64_t i=0;i<N1;i++) {
        const DOUBLE x1pos = *x1++ + off_xwrap;
        const DOUBLE y1pos = *y1++ + off_ywrap;
        const DOUBLE z1pos = *z1++ + off_zwrap;
        for(int w = 0; w < pair.num_weights; w++){
            // local_w1.weights[w] is a pointer to a float in the particle list of weights,
            // just as x0 is a pointer into the list of x-positions.
            // The advancement of the local_w1.weights[w] pointer should always mirror x0.
            pair.weights0[w].a = AVX_SET_FLOAT(*(local_w1.weights[w])++);
        }

        int64_t j;
        if(same_cell == 1) {
            z2++; n_off++;
            j = i+1;
        } else {
            for(;prev_j<N2;prev_j++) {
                const DOUBLE dz = *z2 - z1pos;
                if(dz > -rmax) break;
                z2++; n_off++;
            }
            if(prev_j==N2) break;
            j=prev_j;
        }
        DOUBLE *localz2 = z2;
        DOUBLE *localx2 = x2 + n_off;
        DOUBLE *localy2 = y2 + n_off;
        for(int w = 0; w < local_w2.num_weights; w++){
            local_w2.weights[w] = weights2->weights[w] + n_off;
        }


        for(;j<=(N2-AVX_NVEC);j+=AVX_NVEC) {
            const AVX_FLOATS m_xpos    = AVX_SET_FLOAT(x1pos);
            const AVX_FLOATS m_ypos    = AVX_SET_FLOAT(y1pos);
            const AVX_FLOATS m_zpos    = AVX_SET_FLOAT(z1pos);
            
            union int8 {
                AVX_INTS m_ibin;
                int ibin[AVX_NVEC];
            };
            union int8 union_rbin;
            
            union float8{
                AVX_FLOATS m_Dperp;
                DOUBLE Dperp[AVX_NVEC];
            };
            union float8 union_mDperp;

            const AVX_FLOATS m_x2 = AVX_LOAD_FLOATS_UNALIGNED(localx2);
            const AVX_FLOATS m_y2 = AVX_LOAD_FLOATS_UNALIGNED(localy2);
            const AVX_FLOATS m_z2 = AVX_LOAD_FLOATS_UNALIGNED(localz2);
            
            localx2 += AVX_NVEC;//this might actually exceed the allocated range but we will never dereference that
            localy2 += AVX_NVEC;
            localz2 += AVX_NVEC;
            
            for(int w = 0; w < pair.num_weights; w++){
                pair.weights1[w].a = AVX_LOAD_FLOATS_UNALIGNED(local_w2.weights[w]);
                local_w2.weights[w] += AVX_NVEC;
            }
            
            union float8_weights{
                AVX_FLOATS m_weights;
                DOUBLE weights[NVEC];
            };
            union float8_weights union_mweight;

            const AVX_FLOATS m_rmax = AVX_SET_FLOAT(rmax);
            
            const AVX_FLOATS m_xdiff = AVX_SUBTRACT_FLOATS(m_x2, m_xpos);  //(x[j] - x0)
            const AVX_FLOATS m_ydiff = AVX_SUBTRACT_FLOATS(m_y2, m_ypos);  //(y[j] - y0)
            const AVX_FLOATS m_zdiff = AVX_SUBTRACT_FLOATS(m_z2, m_zpos);  //z2[j:j+NVEC-1] - z1
        
            if(need_weightavg){
                pair.dx.a = m_xdiff;
                pair.dy.a = m_ydiff;
                pair.dz.a = m_zdiff;
            }
            
            const AVX_FLOATS m_sqr_xdiff = AVX_SQUARE_FLOAT(m_xdiff);  //(x0 - x[j])^2
            const AVX_FLOATS m_sqr_ydiff = AVX_SQUARE_FLOAT(m_ydiff);  //(y0 - y[j])^2
            const AVX_FLOATS m_sqr_zdiff = AVX_SQUARE_FLOAT(m_zdiff);
            AVX_FLOATS r2  = AVX_ADD_FLOATS(m_sqr_xdiff,AVX_ADD_FLOATS(m_sqr_ydiff, m_sqr_zdiff));
            
            AVX_FLOATS m_mask_left;
            
            //Do all the distance cuts using masks here in new scope
            {
                //the z2 arrays are sorted in increasing order. which means
                //the z2 value will increase in any future iteration of j.
                //that implies the zdiff values are also monotonically increasing
                //Therefore, if none of the zdiff values are less than rmax, then
                //no future iteration in j can produce a zdiff value less than rmax.
                AVX_FLOATS m_mask_rmax = AVX_COMPARE_FLOATS(m_zdiff,m_rmax,_CMP_LT_OS);
                if(AVX_TEST_COMPARISON(m_mask_rmax) == 0) {
                    //None of the dz^2 values satisfies dz^2 < rmax^2
                    // => no pairs can be added -> continue and process the next NVEC
                    j=N2;
                    break;
                }
                
                const AVX_FLOATS m_pimax_mask = AVX_COMPARE_FLOATS(r2, m_rupp_sqr[nbin-1], _CMP_LT_OS);
                const AVX_FLOATS m_rmin_mask = AVX_COMPARE_FLOATS(r2, m_rupp_sqr[0], _CMP_GE_OS);
                const AVX_FLOATS m_r_mask = AVX_BITWISE_AND(m_pimax_mask,m_rmin_mask);
                
                //Create a combined mask by bitwise and of m1 and m_mask_left.
                //This gives us the mask for all sqr_rmin <= r2 < sqr_rmax
                m_mask_left = AVX_BITWISE_AND(m_mask_rmax,m_r_mask);
                
                //If not, continue with the next iteration of j-loop
                if(AVX_TEST_COMPARISON(m_mask_left) == 0) {
                    continue;
                }
                
                //There is some r2 that satisfies sqr_rmin <= r2 < sqr_rmax && 0.0 <= dz^2 < rmax^2.
                r2 = AVX_BLEND_FLOATS_WITH_MASK(m_rupp_sqr[nbin-1], r2, m_mask_left);
            }
            
            AVX_FLOATS m_rbin = AVX_SET_FLOAT(ZERO);
            if(need_ravg) {
                union_mDperp.m_Dperp = AVX_SQRT_FLOAT(r2);
            }
            if(need_weightavg){
                union_mweight.m_weights = avx_weight_func(&pair);
            }

            //Loop backwards through nbins. m_mask_left contains all the points that are less than rmax
            for(int kbin=nbin-1;kbin>=1;kbin--) {
                const AVX_FLOATS m1 = AVX_COMPARE_FLOATS(r2,m_rupp_sqr[kbin-1],_CMP_GE_OS);
                const AVX_FLOATS m_bin_mask = AVX_BITWISE_AND(m1,m_mask_left);
                const int test2  = AVX_TEST_COMPARISON(m_bin_mask);
                npair[kbin] += AVX_BIT_COUNT_INT(test2);
                if(need_ravg || need_weightavg) {
                    m_rbin = AVX_BLEND_FLOATS_WITH_MASK(m_rbin,m_kbin[kbin], m_bin_mask);
                }
                m_mask_left = AVX_COMPARE_FLOATS(r2,m_rupp_sqr[kbin-1],_CMP_LT_OS);
                const int test3 = AVX_TEST_COMPARISON(m_mask_left);
                if(test3 == 0) {
                    break;
                }
            }

            if(need_ravg || need_weightavg) {
                union_rbin.m_ibin = AVX_TRUNCATE_FLOAT_TO_INT(m_rbin);
                //protect the unroll pragma in case compiler is not icc.
#if  __INTEL_COMPILER
#pragma unroll(AVX_NVEC)
#endif
                for(int jj=0;jj<AVX_NVEC;jj++) {
                    const int kbin = union_rbin.ibin[jj];
                    if(need_ravg){
                        const DOUBLE r = union_mDperp.Dperp[jj];
                        ravg[kbin] += r;
                    }
                    if(need_weightavg){
                        const DOUBLE weight = union_mweight.weights[jj];
                        weightavg[kbin] += weight;
                    }
                }
            }
        }//end of j-loop

        //remainder loop 
        for(;j<N2;j++){
            const DOUBLE dz = *localz2++ - z1pos;
            const DOUBLE dx = *localx2++ - x1pos;
            const DOUBLE dy = *localy2++ - y1pos;
            for(int w = 0; w < pair.num_weights; w++){
                pair.weights1[w].d = *local_w2.weights[w]++;
            }

            if(dz >= rmax) {
                break;
            } 
            
            const DOUBLE r2 = dx*dx + dy*dy + dz*dz;
            if(r2 >= sqr_rmax || r2 < sqr_rmin) {
                continue;
            }
            
            if(need_weightavg){
                pair.dx.d = dx;
                pair.dy.d = dy;
                pair.dz.d = dz;
            }
            
            DOUBLE r, pairweight;//intentionally not initialized to 0.0 -> accidental addition will trigger test failures
            if(need_ravg) {
                r = SQRT(r2);
            }
            if(need_weightavg){
                pairweight = fallback_weight_func(&pair);
            }

            for(int kbin=nbin-1;kbin>=1;kbin--) {
                if(r2 >= rupp_sqr[kbin-1]) {
                    npair[kbin]++;
                    if(need_ravg) {
                        ravg[kbin] += r;
                    }
                    if(need_weightavg){
                        weightavg[kbin] += pairweight;
                    }
                    break;
                }
            }
        }//remainder loop over cellstruct second
    }//loop over cellstruct first

	for(int i=0;i<nbin;i++) {
		src_npairs[i] += npair[i];
        if(need_ravg) {
          src_ravg[i] += ravg[i];
        }
        if(need_weightavg) {
          src_weightavg[i] += weightavg[i];
        }
    }

    return EXIT_SUCCESS;
}
#endif //__AVX__



#if defined (__SSE4_2__)
#include "sse_calls.h"

static inline int xi_sse_intrinsics_DOUBLE(DOUBLE *x0, DOUBLE *y0, DOUBLE *z0, const weight_struct_DOUBLE *weights0, const int64_t N0,
                                           DOUBLE *x1, DOUBLE *y1, DOUBLE *z1, const weight_struct_DOUBLE *weights1, const int64_t N1, const int same_cell, 
                                           const DOUBLE sqr_rmax, const DOUBLE sqr_rmin, const int nbin, const DOUBLE *rupp_sqr, const DOUBLE rmax,
                                           const DOUBLE off_xwrap, const DOUBLE off_ywrap, const DOUBLE off_zwrap
                                           ,DOUBLE *src_ravg, uint64_t *src_npairs,
                                           DOUBLE *src_weightavg, const weight_method_t weight_method)
{
    uint64_t npairs[nbin];
    SSE_FLOATS m_rupp_sqr[nbin];
    for(int i=0;i<nbin;i++) {
        npairs[i] = ZERO;
        m_rupp_sqr[i] = SSE_SET_FLOAT(rupp_sqr[i]);
    }
    
    const int32_t need_ravg = src_ravg != NULL;
    const int32_t need_weightavg = src_weightavg != NULL;
    DOUBLE ravg[nbin], weightavg[nbin];
    SSE_FLOATS m_kbin[nbin];
    if(need_ravg || need_weightavg){
        for(int i=0;i<nbin;i++) {
            m_kbin[i] = SSE_SET_FLOAT((DOUBLE) i);
            if(need_ravg) {
                ravg[i] = ZERO;
            }
            if(need_weightavg){
                weightavg[i] = ZERO;
            }
        }
    }
    
    // A copy whose pointers we can advance
    weight_struct_DOUBLE local_w0 = {.weights={NULL}, .num_weights=0}, 
                         local_w1 = {.weights={NULL}, .num_weights=0};
    pair_struct_DOUBLE pair = {.num_weights=0};
    sse_weight_func_t_DOUBLE sse_weight_func = NULL;
    weight_func_t_DOUBLE fallback_weight_func = NULL;
    if(need_weightavg){
      // Same particle list, new copy of num_weights pointers into that list
      local_w0 = *weights0;
      local_w1 = *weights1;
      
      pair.num_weights = local_w0.num_weights;
      
      sse_weight_func = get_sse_weight_func_by_method_DOUBLE(weight_method);
      fallback_weight_func = get_weight_func_by_method_DOUBLE(weight_method);
    }


    int64_t prev_j=0, n_off = 0;
    for(int64_t i=0;i<N0;i++) {
        const DOUBLE xpos = *x0++ + off_xwrap;
        const DOUBLE ypos = *y0++ + off_ywrap;
        const DOUBLE zpos = *z0++ + off_zwrap;
        for(int w = 0; w < pair.num_weights; w++){
            // local_w0.weights[w] is a pointer to a float in the particle list of weights,
            // just as x0 is a pointer into the list of x-positions.
            // The advancement of the local_w0.weights[w] pointer should always mirror x0.
            pair.weights0[w].s = SSE_SET_FLOAT(*local_w0.weights[w]++);
        }

        int64_t j;
        if(same_cell == 1) {
            z1++; n_off++;
            j = i+1;
        } else {
            for(;prev_j<N1;prev_j++) {
                const DOUBLE dz = *z1 - zpos;
                if(dz > -rmax) break;
                z1++; n_off++;
            }
            if(prev_j == N1) {
                i=N0;
                break;
            }
            j = prev_j;
        }
        DOUBLE *localz1 = z1;
        DOUBLE *localx1 = x1 + n_off;
        DOUBLE *localy1 = y1 + n_off;
        for(int w = 0; w < local_w1.num_weights; w++){
            local_w1.weights[w] = weights1->weights[w] + n_off;
        }

		for(;j<=(N1 - SSE_NVEC);j+=SSE_NVEC){
            union int4{
                SSE_INTS m_ibin;
                int ibin[SSE_NVEC];
            };
            union int4 union_rbin;
            
            union float4{
                SSE_FLOATS m_Dperp;
                DOUBLE Dperp[SSE_NVEC];
            };
            union float4 union_mDperp;

            const SSE_FLOATS m_xpos = SSE_SET_FLOAT(xpos);
            const SSE_FLOATS m_ypos = SSE_SET_FLOAT(ypos);
            const SSE_FLOATS m_zpos = SSE_SET_FLOAT(zpos);

            const SSE_FLOATS m_x1 = SSE_LOAD_FLOATS_UNALIGNED(localx1);
			const SSE_FLOATS m_y1 = SSE_LOAD_FLOATS_UNALIGNED(localy1);
			const SSE_FLOATS m_z1 = SSE_LOAD_FLOATS_UNALIGNED(localz1);

			localx1 += SSE_NVEC;
			localy1 += SSE_NVEC;
			localz1 += SSE_NVEC;
            
            for(int w = 0; w < pair.num_weights; w++){
                pair.weights1[w].s = SSE_LOAD_FLOATS_UNALIGNED(local_w1.weights[w]);
                local_w1.weights[w] += SSE_NVEC;
            }

            union float4_weights{
                SSE_FLOATS m_weights;
                DOUBLE weights[SSE_NVEC];
            };
            union float4_weights union_mweight;

            const SSE_FLOATS m_rmax = SSE_SET_FLOAT(rmax);
			const SSE_FLOATS m_sqr_rmax = SSE_SET_FLOAT(sqr_rmax);
			const SSE_FLOATS m_sqr_rmin = SSE_SET_FLOAT(sqr_rmin);
			

            const SSE_FLOATS m_xdiff = SSE_SUBTRACT_FLOATS(m_x1, m_xpos);  //(x[j] - x0)
            const SSE_FLOATS m_ydiff = SSE_SUBTRACT_FLOATS(m_y1, m_ypos);  //(y[j] - y0)
            const SSE_FLOATS m_zdiff = SSE_SUBTRACT_FLOATS(m_z1, m_zpos);  //z2[j:j+NVEC-1] - z1
                
            const SSE_FLOATS m_sqr_xdiff = SSE_SQUARE_FLOAT(m_xdiff);
            const SSE_FLOATS m_sqr_ydiff = SSE_SQUARE_FLOAT(m_ydiff);
            const SSE_FLOATS m_sqr_zdiff = SSE_SQUARE_FLOAT(m_zdiff);
            
            if(need_weightavg){
                pair.dx.s = m_xdiff;
                pair.dy.s = m_ydiff;
                pair.dz.s = m_zdiff;
            }
                        
            SSE_FLOATS r2  = SSE_ADD_FLOATS(m_sqr_zdiff,SSE_ADD_FLOATS(m_sqr_xdiff,m_sqr_ydiff));
            SSE_FLOATS m_mask_left;
			{
                const SSE_FLOATS m_pimax_mask = SSE_COMPARE_FLOATS_LT(m_zdiff,m_rmax);
                if(SSE_TEST_COMPARISON(m_pimax_mask) == 0) {
                    j = N1;
                    break;
                }
                
                const SSE_FLOATS m_rmin_mask = SSE_COMPARE_FLOATS_GE(r2, m_sqr_rmin);
                const SSE_FLOATS m_rmax_mask = SSE_COMPARE_FLOATS_LT(r2,m_sqr_rmax);
                const SSE_FLOATS m_r_mask = SSE_BITWISE_AND(m_rmin_mask, m_rmax_mask);
                m_mask_left = SSE_BITWISE_AND(m_pimax_mask, m_r_mask);
				if(SSE_TEST_COMPARISON(m_mask_left) == 0) {
					continue;
				}
				r2 = SSE_BLEND_FLOATS_WITH_MASK(m_sqr_rmax, r2, m_mask_left);
            }
                
            SSE_FLOATS m_rbin;
            if(need_ravg) {
                union_mDperp.m_Dperp = SSE_SQRT_FLOAT(r2);
                m_rbin = SSE_SET_FLOAT(ZERO);
            }
            if(need_weightavg){
                union_mweight.m_weights = sse_weight_func(&pair);
            }

			for(int kbin=nbin-1;kbin>=1;kbin--) {
				SSE_FLOATS m1 = SSE_COMPARE_FLOATS_GE(r2,m_rupp_sqr[kbin-1]);
				SSE_FLOATS m_bin_mask = SSE_BITWISE_AND(m1,m_mask_left);
				m_mask_left = SSE_COMPARE_FLOATS_LT(r2,m_rupp_sqr[kbin-1]);
				int test2  = SSE_TEST_COMPARISON(m_bin_mask);
				npairs[kbin] += SSE_BIT_COUNT_INT(test2);
                if(need_ravg || need_weightavg) {
                    m_rbin = SSE_BLEND_FLOATS_WITH_MASK(m_rbin,m_kbin[kbin], m_bin_mask);
                }
				int test3 = SSE_TEST_COMPARISON(m_mask_left);
				if(test3 == 0) {
					break;
				}
			}

            if(need_ravg || need_weightavg) {
                union_rbin.m_ibin = SSE_TRUNCATE_FLOAT_TO_INT(m_rbin);
                //protect the unroll pragma in case compiler is not icc.
#if  __INTEL_COMPILER
#pragma unroll(SSE_NVEC)
#endif
                for(int jj=0;jj<SSE_NVEC;jj++) {
                    const int kbin = union_rbin.ibin[jj];
                    if(need_ravg){
                        const DOUBLE r = union_mDperp.Dperp[jj];
                        ravg[kbin] += r;
                    }
                    if(need_weightavg){
                        const DOUBLE weight = union_mweight.weights[jj];
                        weightavg[kbin] += weight;
                    }
                }
            }//need_ravg
		}//j-loop			

		for(;j<N1;j++) {
			const DOUBLE dx = *localx1++ - xpos;
			const DOUBLE dy = *localy1++ - ypos;
			const DOUBLE dz = *localz1++ - zpos;
            for(int w = 0; w < pair.num_weights; w++){
                pair.weights1[w].d = *local_w1.weights[w]++;
            }

            if(dz >= rmax) break;
            
			const DOUBLE r2 = dx*dx + dy*dy + dz*dz;
			if(r2 >= sqr_rmax || r2 < sqr_rmin) continue;
            
            if(need_weightavg){
                pair.dx.d = dx;
                pair.dy.d = dy;
                pair.dz.d = dz;
            }

            DOUBLE r, pairweight;//intentionally not initialized to 0.0 -> accidental addition will trigger test failures
            if(need_ravg) {
                r = SQRT(r2);
            }
            if(need_weightavg){
            pairweight = fallback_weight_func(&pair);
            }

			for(int kbin=nbin-1;kbin>=1;kbin--){
				if(r2 >= rupp_sqr[kbin-1]) {
					npairs[kbin]++;
                    if(need_ravg) {
                        ravg[kbin] += r;
                    }
                    if(need_weightavg){
                        weightavg[kbin] += pairweight;
                    }
					break;
				}
			}//searching for kbin loop
		}
    }

	for(int i=0;i<nbin;i++) {
		src_npairs[i] += npairs[i];
        if(need_ravg) {
            src_ravg[i] += ravg[i];
        }
        if(need_weightavg) {
            src_weightavg[i] += weightavg[i];
        }
	}

    return EXIT_SUCCESS;
}

#endif //__SSE4_2__

static inline int xi_fallback_DOUBLE(DOUBLE *x0, DOUBLE *y0, DOUBLE *z0, const weight_struct_DOUBLE *weights0, const int64_t N0,
                                     DOUBLE *x1, DOUBLE *y1, DOUBLE *z1, const weight_struct_DOUBLE *weights1, const int64_t N1, const int same_cell, 
                                     const DOUBLE sqr_rmax, const DOUBLE sqr_rmin, const int nbin, const DOUBLE *rupp_sqr, const DOUBLE rmax,
                                     const DOUBLE off_xwrap, const DOUBLE off_ywrap, const DOUBLE off_zwrap,
                                     DOUBLE *src_ravg, uint64_t *src_npairs,
                                     DOUBLE *src_weightavg, const weight_method_t weight_method)
{
    /*----------------- FALLBACK CODE --------------------*/
    uint64_t npairs[nbin];
    for(int i=0;i<nbin;i++) {
        npairs[i]=0;
    }

    const int32_t need_ravg = src_ravg != NULL;
    const int32_t need_weightavg = src_weightavg != NULL;
    DOUBLE ravg[nbin], weightavg[nbin];
    for(int i=0;i<nbin;i++) {
        if(need_ravg) {
            ravg[i]=0.;
        }
        if(need_weightavg){
            weightavg[i]=0.;
        }
    }
    
    // A copy whose pointers we can advance
    weight_struct_DOUBLE local_w0 = {.weights={NULL}, .num_weights=0}, 
                         local_w1 = {.weights={NULL}, .num_weights=0};
    pair_struct_DOUBLE pair = {.num_weights=0};
    weight_func_t_DOUBLE weight_func = NULL;
    if(need_weightavg){
      // Same particle list, new copy of num_weights pointers into that list
      local_w0 = *weights0;
      local_w1 = *weights1;
      
      pair.num_weights = local_w0.num_weights;
      
      weight_func = get_weight_func_by_method_DOUBLE(weight_method);
    }


    /* naive implementation that is guaranteed to compile */
    int64_t nleft=N1, n_off = 0;
    for(int64_t i=0;i<N0;i++) {
        const DOUBLE xpos = *x0++ + off_xwrap;
        const DOUBLE ypos = *y0++ + off_ywrap;
        const DOUBLE zpos = *z0++ + off_zwrap;
        for(int w = 0; w < pair.num_weights; w++){
            pair.weights0[w].d = *local_w0.weights[w]++;
        }

        /* If in the same cell, unique pairs are guaranteed by not including the current particle */
        if(same_cell == 1) {
            z1++; n_off++;
            nleft--;
        } else {
            /* For a different cell, all pairs are unique pairs, since two cells are only opened for pairs once (accounted for in the assign_ngb_cells function)*/
            while(nleft > 0) {
                /*Particles are sorted on 'z', in increasing order */
                const DOUBLE dz = *z1 - zpos;
                if(dz > -rmax) break;
                z1++; n_off++;
                nleft--;
            }
            /*If no particle in the second cell satisfies distance constraints on 'dz' for the current 'i'th particle in first cell, 
              then there can be no more pairs from any particles in the first cell (since the first cell is also sorted in increasing order in 'z')
             */
            if(nleft == 0) {
                i=N0;
                break;
            }
        }
        DOUBLE *localz1 = z1;
        DOUBLE *localx1 = x1 + n_off;
        DOUBLE *localy1 = y1 + n_off;
        for(int w = 0; w < pair.num_weights; w++){
            local_w1.weights[w] = weights1->weights[w] + n_off;
        }
        
        for(int64_t j=0;j<nleft;j++) {
            const DOUBLE dx = *localx1++ - xpos;
            const DOUBLE dy = *localy1++ - ypos;
            const DOUBLE dz = *localz1++ - zpos;
            for(int w = 0; w < pair.num_weights; w++){
                pair.weights1[w].d = *local_w1.weights[w]++;
            }

            if(dz >=rmax) break;
            
            const DOUBLE r2 = dx*dx + dy*dy + dz*dz;
            if(r2 >= sqr_rmax || r2 < sqr_rmin) continue;
            
            if(need_weightavg){
                pair.dx.d = dx;
                pair.dy.d = dy;
                pair.dz.d = dz;
            }
            
            DOUBLE r, pairweight;//intentionally not initialized to 0.0 -> accidental addition will trigger test failures (or valgrind failures)
            if(need_ravg) {
                r = SQRT(r2);
            }
            if(need_weightavg){
                pairweight = weight_func(&pair);
            }
            
            for(int kbin=nbin-1;kbin>=1;kbin--){
                if(r2 >= rupp_sqr[kbin-1]) {
                    npairs[kbin]++;
                    if(need_ravg) {
                        ravg[kbin] += r;
                    }
                    if(need_weightavg){
                        weightavg[kbin] += pairweight;
                    }
                    break;
                }
            }//searching for kbin loop                                                               
        }
    }

    for(int i=0;i<nbin;i++) {
        src_npairs[i] += npairs[i];
        if(need_ravg) {
            src_ravg[i] += ravg[i];
        }
        if(need_weightavg){
            src_weightavg[i] += weightavg[i];
        }
    }
    /*----------------- FALLBACK CODE --------------------*/
    return EXIT_SUCCESS;
}
