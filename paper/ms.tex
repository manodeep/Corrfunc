\documentclass[apj]{emulateapj}

\usepackage{amsmath}
\usepackage{aas_macros}
\usepackage{natbib}
\usepackage{rotating}
\usepackage{xspace}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{color}
\usepackage{listings}

%%% Definitions added by Manodeep 
\newcommand{\Msun}{M_{\odot}}
\newcommand{\hMsun}{h^{-1}M_{\odot}}
\newcommand{\sdss}{{\small{SDSS}}\xspace}
\newcommand{\hMpc}{\ensuremath{{h^{-1}Mpc}\xspace}}
\newcommand{\hkpc}{\ensuremath{{h^{-1}kpc}\xspace}}
\newcommand{\emcee}{\texttt emcee}
\newcommand{\todo}[1]{\marginpar{TODO}{\color{red}#1}}


%%\setstretch{2.0}
%% \usepackage{etoolbox}
%% \makeatletter
%% \patchcmd{\@makecaption}
%%   {\scshape}
%%   {}
%%   {}
%%   {}
%% \makeatother


\lstset{
  language=C,                % choose the language of the code
  numbers=left,                   % where to put the line-numbers
  stepnumber=1,                   % the step between two line-numbers.
  numbersep=5pt,                  % how far the line-numbers are from the code
  backgroundcolor=\color{white},  % choose the background color. You must add \usepackage{color}
  showspaces=false,               % show spaces adding particular underscores
  showstringspaces=false,         % underline spaces within strings
  showtabs=false,                 % show tabs within strings adding particular underscores
  tabsize=2,                      % sets default tabsize to 2 spaces
  captionpos=b,                   % sets the caption-position to bottom
  breaklines=true,                % sets automatic line breaking
  breakatwhitespace=true,         % sets if automatic breaks should only happen at whitespace
  title=\lstname,                 % show the filename of files included with \lstinputlisting;
}

\begin{document}

\title{Cache is King: Presenting a Suite of Blazing-Fast, Correlation Function Code}

\author{Manodeep Sinha\altaffilmark{1}}
\affiliation{Department of Physics and Astronomy, Vanderbilt University, Nashville, TN, 37235}
\altaffiltext{1}{manodeep.sinha@vanderbilt.edu}

\begin{abstract}
We have entered the era of `Big Data' where we are gathering a tremendous amount of data that
needs to be processed as well as modeled accurately. In particular, large galaxy surveys
like the Sloan Digitial Sky Survey requires computing correlation functions. While measuring the
correlation function in the data is not a bottle-neck, modeling the observed galaxy distribution correctly
{\em} requires a MCMC chain and repeated measurements of $\xi(r)$ and/or $\xi(r_p,\pi)$. However, 
the architecture of modern CPU's means that best performance is only obtained when cache misses 
are kept to a minimum. Here, I present a suite of 3 correlation function codes that exploit 
current CPU architecture and hand-written AVX  


The accompanying codes are publicly available at \url{https://bitbucket.org/manodeep/Corrfunc/}. 
\end{abstract}

\keywords{methods: numerical}

\maketitle


\section{INTRODUCTION}
%% MS: I have not encountered this in the past -- but the footnotecounter needs to be reset. Otherwise, 
%% the numbering starts at Nauthors+1. Presumably \altaffiltext uses \footnote internally and does not
%% reset the counter like a responsible person!
\setcounter{footnote}{0}
The large-scale structure of the Universe can be now measured using $\mathcal{O}(million)$ galaxies 
using data from current surveys like SDSS/BOSS surveys. Upcoming surveys like LSST will probe even 
deeper and wider and aim to target on $\mathcal{O}(10's of million)$ galaxies. With such a large galaxy data, 
we can measure the galaxy density field fairly accurately in the data. While the number of galaxies observed 
in these current and upcoming surveys is large, we have to compute the galaxy density fields {\em only once}. Thus, 
even a slow, correlation function code will be fine when measuring the data correlation function. 
However, predicting this galaxy  density field data will require an even larger number of model galaxies -- increasing the computational 
load of determining the density field even further. Typically, the modeling process also involves an MCMC -- 
requiring $\sim$ millions of evaluations of the correlation function.\footnote{I will assume that creating the models themselves 
is much faster compared to computing the correlation functions.} 

Modern cpu's have a hierarchy of memory locations; the smallest and fastest are closest to the cores while the largest and slowest are the
farthest. All cpu instructions need to be carried out from cpu registers - there are $\sim 10-30$ registers typically available and the 
access times can be thought of as instantaneous. Next up is the $L1$ cache divided into $L1D$ for data and $L1I$ for instructions cache. Since 
the cpu always necessarily executes instructions that are close together, we will ignore the instruction cache from now on. Typical $L1$ cache 
sizes range from 64KB to 128 KB. Next level up is the $L2$ cache, typically $\sim$ 256KB to 1 MB. The last level cache or the $L3$ cache 
is usually shared across all cores on the socket and can be 10 MB to 40 MB. 


\section{Methods}
We need to compute pairwise distances to get the correlation function. A naive implementation of a correlation function would compute {\em all possible} 
pairwise separations with a complexity $\mathcal{O}(N^2)$. However, for almost all correlation functions, we are only interested in separations less than 
a certain \rmax, where \rmax is much smaller than the domain of the point distribution itself. We can then immediately see a way to prune pairs that can not
{\em possibly} be within \rmax. If we impose a 3-d grid, with cell-size \rmax, then two points separated by more than one cell size (\rmax) in any one 
dimension can not be within \rmax of each other. Thus, given one point which is the target galaxy and a grid with cell-size \rmax, 
immediately allows us to prune {\em all} of the points that are not within 1 cell offset in each dimension. However, even with this pruning, the actual 
implementation of the algorithm matters. For instance, a linked-list in each cell performs $\sim 60x$ worse than the algorithm described here.\footnote{The 
usual linked-list is very cache-unfriendly. Each dereference requires a read from a new region of memory and an almost guaranteed cache miss. } 

\subsection{How to Maintain Cache Locality within the Grid}
For all pairs around a given target galaxy, we need to compute distances to all points within all neighbouring 3-d cells. 
We ensure that the particle locations are contiguous by moving them into the following \texttt{C struct} in the order in which they arrive. 



\footnote{Most of the code assumes that the number of galaxies is within INT_MAX $\sim$ 2.1 billion. So, it would appear 
to make no sense that the field inside the struct cell has `nelements' of type int64_t. The reason I have 
chosen to use int64_t is that under C alignment rules 4 bytes of padding would have appended anyway to the struct cell. So, I chose 
to declare `nelements' as an int64_t and used those 4 bytes. Saves some future conversion/coding. }





\subsection{Hand-written Vectorization Support}
Advanced Vector Extensions (AVX) has been available in cpu's more recent than 2011. AVX allows the processing of 8 floats or 4 double simultaneously -- thus, 
potentially increasing the throughput by a factor of 8x/4x. However, automatic vectorization is not always possible by the compiler and in those cases 
we can write AVX vector intrinsics to directly manipulate 8 floats/4 doubles. 


\subsection{Comparison to Tree Algorithms}

\section{Results}

\section{Benchmarks \& Scaling}

\section{Conclusions}

\acknowledgements 


\section{}


\bibliographystyle{apj}
\bibliography{master}

\end{document}




